<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="false" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>lesson3_基于 InternLM 和 LangChain 搭建自己的知识库 | aqizhoua</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="大模型开发范式LLM大模型的局限性 知识时效性受限：如何让LLM能够获取最新的知识 专业能力有限：如何打造垂域大模型 定制化成本高：如何打造个人专属的LLM应用  两种思路（开发范式） RAG（检索增强生成） （核心思想：给大模型外挂知识库）  无需重新训练，能够实时更新  上限取决于基座大模型  需要大型的上下文，对于总结归纳任务表现不佳  RAG原理： 对于每个用户的输入，使用向量模型sent">
<meta property="og:type" content="article">
<meta property="og:title" content="lesson3_基于 InternLM 和 LangChain 搭建自己的知识库">
<meta property="og:url" content="https://aqizhoua.github.io/poster/dd86f549.html">
<meta property="og:site_name" content="aqizhoua">
<meta property="og:description" content="大模型开发范式LLM大模型的局限性 知识时效性受限：如何让LLM能够获取最新的知识 专业能力有限：如何打造垂域大模型 定制化成本高：如何打造个人专属的LLM应用  两种思路（开发范式） RAG（检索增强生成） （核心思想：给大模型外挂知识库）  无需重新训练，能够实时更新  上限取决于基座大模型  需要大型的上下文，对于总结归纳任务表现不佳  RAG原理： 对于每个用户的输入，使用向量模型sent">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/1.3top0estmqg0.webp">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/2.3am7c0yqoq40.webp">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/5.2oufhlsds6g0.webp">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/7.4jhwdub0jde0.webp">
<meta property="article:published_time" content="2024-01-14T04:49:24.000Z">
<meta property="article:modified_time" content="2024-01-14T06:52:23.199Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="书生浦语大模型实战营">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/1.3top0estmqg0.webp">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  
   
  <div id="main-grid" class="shadow   ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>aqizhoua </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/fanren">Fanren</a>
    
      <a class="main-nav-link" href="/geek">geek</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/fanren">Fanren</a>
    
      <a class="nav-dropdown-link" href="/geek">geek</a>
    
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/sleep.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">aqizhoua </div>
      <div class="dot"></div>
      <div class="subtitle">人生是旷野 </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/aqizhoua" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Categories</h3>
      <div class="category-box"></div>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Tags</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/MOT/" rel="tag">MOT</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/hexo/" rel="tag">hexo</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/smtp/" rel="tag">smtp</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5/" rel="tag">书生浦语大模型实战营</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/" rel="tag">数学知识</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" rel="tag">每日一题</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Archives</h3>
      
      
        <a class="archive-link" href="/archives/2024/02 ">
          February 2024 
          <div class="archive-count">3 </div>
        </a>
      
        <a class="archive-link" href="/archives/2024/01 ">
          January 2024 
          <div class="archive-count">14 </div>
        </a>
      
        <a class="archive-link" href="/archives/2023/11 ">
          November 2023 
          <div class="archive-count">5 </div>
        </a>
      
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Recent Posts</h3>
      <ul>
        
          <a class="recent-link" href="/poster/4bdca450.html" title="lesson4作业" >
            <div class="recent-link-text">
              lesson4作业
            </div>
          </a>
        
          <a class="recent-link" href="/poster/290cb30.html" title="25. K 个一组翻转链表" >
            <div class="recent-link-text">
              25. K 个一组翻转链表
            </div>
          </a>
        
          <a class="recent-link" href="/poster/7c2a4ef3.html" title="1690. 石子游戏 VII" >
            <div class="recent-link-text">
              1690. 石子游戏 VII
            </div>
          </a>
        
          <a class="recent-link" href="/poster/2b0ec29a.html" title="2808. 使循环数组所有元素相等的最少秒数" >
            <div class="recent-link-text">
              2808. 使循环数组所有元素相等的最少秒数
            </div>
          </a>
        
          <a class="recent-link" href="/poster/7082c407.html" title="2861. 最大合金数" >
            <div class="recent-link-text">
              2861. 最大合金数
            </div>
          </a>
        
      </ul>
    </div>
  </div>

    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-lesson3-基于-InternLM-和-LangChain-搭建自己的知识库" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        lesson3_基于 InternLM 和 LangChain 搭建自己的知识库
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-01-14T04:49:24.000Z" itemprop="datePublished">2024-01-14</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            8.2k words 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5/" rel="tag">书生浦语大模型实战营</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h2 id="大模型开发范式"><a href="#大模型开发范式" class="headerlink" title="大模型开发范式"></a>大模型开发范式</h2><h3 id="LLM大模型的局限性"><a href="#LLM大模型的局限性" class="headerlink" title="LLM大模型的局限性"></a>LLM大模型的局限性</h3><ul>
<li>知识时效性受限：如何让LLM能够获取最新的知识</li>
<li>专业能力有限：如何打造垂域大模型</li>
<li>定制化成本高：如何打造个人专属的LLM应用</li>
</ul>
<h3 id="两种思路（开发范式）"><a href="#两种思路（开发范式）" class="headerlink" title="两种思路（开发范式）"></a>两种思路（开发范式）</h3><ul>
<li><p>RAG（检索增强生成）</p>
<p>（核心思想：给大模型外挂知识库）</p>
<ul>
<li><p>无需重新训练，能够实时更新</p>
</li>
<li><p>上限取决于基座大模型</p>
</li>
<li><p>需要大型的上下文，对于总结归纳任务表现不佳</p>
</li>
<li><p>RAG原理：</p>
<p>对于每个用户的输入，使用向量模型sentence transformer，将输入文本转化为向量，在Chroma向量数据库中寻找匹配的相似文本段，我们认为与问题相似的作为答案，将用户的输入和相似文本段一起嵌入到Prompt，传递给InternLM模型，再做出最终的回答。</p>
<p><img src="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/1.3top0estmqg0.webp" alt></p>
</li>
</ul>
</li>
<li><p>Finetune（核心：在已有数据集上微调）</p>
<ul>
<li>可以进行更多<strong>个性化</strong>微调，以满足用户需求。</li>
<li>需要在新的数据集上进行训练，这可能会导致成本上升。然而，这也意味着模型可以更好地适应不断变化的环境。</li>
<li>需要注意的是，由于训练成本较高，模型的更新可能不是实时的，但我们可以定期进行更新以确保其性能始终保持在最佳状态。</li>
</ul>
</li>
</ul>
<h2 id="LangChain简介"><a href="#LangChain简介" class="headerlink" title="LangChain简介"></a>LangChain简介</h2><h3 id="LangChain"><a href="#LangChain" class="headerlink" title="LangChain"></a>LangChain</h3><h4 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h4><p><code>LangChain</code>框架是一个开源工具，能够为各种<code>LLM</code>提供通用接口来简化应用程序的开发流程，帮助开发者自由构建大模型应用。</p>
<p><code>LangChain</code> 封装了很多组件，通过将这些组件组合能够构建适用于自己的<code>RAG</code> 。一个<code>chain</code>能够封装一系列的<code>LLM</code>操作，详细的可以见后续的检索问答链。</p>
<h4 id="核心组成模块"><a href="#核心组成模块" class="headerlink" title="核心组成模块"></a>核心组成模块</h4><p><code>链（Chains）</code>是将组件组合实现端到端应用，通过一个对象封装实现一系列LLM操作</p>
<p>比如说，检索问答链，覆盖实现RAG（检索增强生成)的全部流程。</p>
<h3 id="InternLM"><a href="#InternLM" class="headerlink" title="InternLM"></a>InternLM</h3><p><code>InternLM</code>是一个通用的基座大模型，可以理解自然语言的指令，能够发展到多元应用，应用于多个领域。</p>
<h2 id="基于LangChain搭建RAG应用"><a href="#基于LangChain搭建RAG应用" class="headerlink" title="基于LangChain搭建RAG应用"></a>基于LangChain搭建RAG应用</h2><p>本地文档→Loader：统一变成text文本格式→分离文本成chunk→开源词向量模型sentence transformer变成向量格式→存储到基于Chroma的向量数据库→匹配搜索→输出</p>
<p><img src="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/2.3am7c0yqoq40.webp" alt="2"></p>
<h3 id="构建向量数据库"><a href="#构建向量数据库" class="headerlink" title="构建向量数据库"></a>构建向量数据库</h3><ol>
<li><strong>格式转化：</strong> 个人的数据类型：txt、pdf、markdown，使用相应的数据加载器转化为无格式的字符串</li>
<li><strong>切分</strong>：由于文档一般很长，超过了限制，需要对文档进行切分成chunk</li>
<li><strong>向量化</strong>：使用<code>Embedding</code>模型进行向量化，后面使用的是<strong>开源词向量模型</strong><code>sentence transformer</code>，支持语义检索的数据库选取的是轻量级的<code>Chroma</code>。</li>
</ol>
<h3 id="搭建知识库助手"><a href="#搭建知识库助手" class="headerlink" title="搭建知识库助手"></a>搭建知识库助手</h3><p>封装一个Langchain的自定义LLM类，调用本地部署的InternLM即可</p>
<h3 id="RAG的改进方案"><a href="#RAG的改进方案" class="headerlink" title="RAG的改进方案"></a>RAG的改进方案</h3><ul>
<li>检索方面：<ul>
<li>基于语义（而不是字符长度）进行分割，保证每一个chunk的语义完整</li>
<li>给每一个chunk生成概括性索引，检索时匹配索引而不是全文相关片段</li>
</ul>
</li>
<li>Prompt方面：<ul>
<li>迭代优化Prompt策略，不断激发模型的潜在能力</li>
</ul>
</li>
</ul>
<h2 id="Web-Demo部署"><a href="#Web-Demo部署" class="headerlink" title="Web Demo部署"></a>Web Demo部署</h2><h3 id="demo设置主要过程及关键代码"><a href="#demo设置主要过程及关键代码" class="headerlink" title="demo设置主要过程及关键代码"></a>demo设置主要过程及关键代码</h3><h4 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h4><ul>
<li><p>环境准备</p>
<ul>
<li><p>开发机创建</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://aicarrier.feishu.cn/wiki/VLS7w5I22iQWmTk0ExpczIKcnpf">https://aicarrier.feishu.cn/wiki/VLS7w5I22iQWmTk0ExpczIKcnpf</a></li>
</ul>
</li>
<li><p>安装conda环境、安装依赖库</p>
</li>
</ul>
</li>
<li>模型下载<ul>
<li>下载internlm-chat-7b模型</li>
</ul>
</li>
<li>LangChain 相关环境配置<ul>
<li>安装LangChain依赖包</li>
<li>下载开源词向量模型 Sentence Transformer</li>
</ul>
</li>
<li>下载NLTK 相关资源<ul>
<li>在使用开源词向量模型构建开源词向量的时候，需要用到第三方库 <code>nltk</code> 的一些资源</li>
</ul>
</li>
<li>下载本项目代码</li>
</ul>
<h4 id="知识库搭建"><a href="#知识库搭建" class="headerlink" title="知识库搭建"></a>知识库搭建</h4><ul>
<li>数据收集</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 进入到数据库盘</span><br><span class="line">cd /root/data</span><br><span class="line"># clone 上述开源仓库</span><br><span class="line">git clone https://gitee.com/open-compass/opencompass.git</span><br><span class="line">git clone https://gitee.com/InternLM/lmdeploy.git</span><br><span class="line">git clone https://gitee.com/InternLM/xtuner.git</span><br><span class="line">git clone https://gitee.com/InternLM/InternLM-XComposer.git</span><br><span class="line">git clone https://gitee.com/InternLM/lagent.git</span><br><span class="line">git clone https://gitee.com/InternLM/InternLM.git</span><br></pre></td></tr></table></figure>
<ul>
<li><p>加载数据</p>
</li>
<li><p>构建向量数据库</p>
</li>
<li><p>整体脚本</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"># 首先导入所需第三方库</span><br><span class="line">from langchain.document_loaders import UnstructuredFileLoader</span><br><span class="line">from langchain.document_loaders import UnstructuredMarkdownLoader</span><br><span class="line">from langchain.text_splitter import RecursiveCharacterTextSplitter</span><br><span class="line">from langchain.vectorstores import Chroma</span><br><span class="line">from langchain.embeddings.huggingface import HuggingFaceEmbeddings</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># 获取文件路径函数</span><br><span class="line">def get_files(dir_path):</span><br><span class="line">    # args：dir_path，目标文件夹路径</span><br><span class="line">    file_list = []</span><br><span class="line">    for filepath, dirnames, filenames in os.walk(dir_path):</span><br><span class="line">        # os.walk 函数将递归遍历指定文件夹</span><br><span class="line">        for filename in filenames:</span><br><span class="line">            # 通过后缀名判断文件类型是否满足要求</span><br><span class="line">            if filename.endswith(&quot;.md&quot;):</span><br><span class="line">                # 如果满足要求，将其绝对路径加入到结果列表</span><br><span class="line">                file_list.append(os.path.join(filepath, filename))</span><br><span class="line">            elif filename.endswith(&quot;.txt&quot;):</span><br><span class="line">                file_list.append(os.path.join(filepath, filename))</span><br><span class="line">    return file_list</span><br><span class="line"></span><br><span class="line"># 加载文件函数</span><br><span class="line">def get_text(dir_path):</span><br><span class="line">    # args：dir_path，目标文件夹路径</span><br><span class="line">    # 首先调用上文定义的函数得到目标文件路径列表</span><br><span class="line">    file_lst = get_files(dir_path)</span><br><span class="line">    # docs 存放加载之后的纯文本对象</span><br><span class="line">    docs = []</span><br><span class="line">    # 遍历所有目标文件</span><br><span class="line">    for one_file in tqdm(file_lst):</span><br><span class="line">        file_type = one_file.split(&#x27;.&#x27;)[-1]</span><br><span class="line">        if file_type == &#x27;md&#x27;:</span><br><span class="line">            loader = UnstructuredMarkdownLoader(one_file)</span><br><span class="line">        elif file_type == &#x27;txt&#x27;:</span><br><span class="line">            loader = UnstructuredFileLoader(one_file)</span><br><span class="line">        else:</span><br><span class="line">            # 如果是不符合条件的文件，直接跳过</span><br><span class="line">            continue</span><br><span class="line">        docs.extend(loader.load())</span><br><span class="line">    return docs</span><br><span class="line"></span><br><span class="line"># 目标文件夹</span><br><span class="line">tar_dir = [</span><br><span class="line">    &quot;/root/data/InternLM&quot;,</span><br><span class="line">    &quot;/root/data/InternLM-XComposer&quot;,</span><br><span class="line">    &quot;/root/data/lagent&quot;,</span><br><span class="line">    &quot;/root/data/lmdeploy&quot;,</span><br><span class="line">    &quot;/root/data/opencompass&quot;,</span><br><span class="line">    &quot;/root/data/xtuner&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># 加载目标文件</span><br><span class="line">docs = []</span><br><span class="line">for dir_path in tar_dir:</span><br><span class="line">    docs.extend(get_text(dir_path))</span><br><span class="line"></span><br><span class="line"># 对文本进行分块</span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=500, chunk_overlap=150)</span><br><span class="line">split_docs = text_splitter.split_documents(docs)</span><br><span class="line"></span><br><span class="line"># 加载开源词向量模型</span><br><span class="line">embeddings = HuggingFaceEmbeddings(model_name=&quot;/root/model/sentence-transformer&quot;)</span><br><span class="line"></span><br><span class="line"># 构建向量数据库</span><br><span class="line"># 定义持久化路径</span><br><span class="line">persist_directory = &#x27;data_base/vector_db/chroma&#x27;</span><br><span class="line"># 加载数据库</span><br><span class="line">vectordb = Chroma.from_documents(</span><br><span class="line">    documents=split_docs,</span><br><span class="line">    embedding=embeddings,</span><br><span class="line">    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上</span><br><span class="line">)</span><br><span class="line"># 将加载的向量数据库持久化到磁盘上</span><br><span class="line">vectordb.persist()</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/5.2oufhlsds6g0.webp" alt="5"></p>
<h4 id="InternLM-接入-LangChain"><a href="#InternLM-接入-LangChain" class="headerlink" title="InternLM 接入 LangChain"></a>InternLM 接入 LangChain</h4><ul>
<li>为便捷构建 LLM 应用，我们需要基于本地部署的 InternLM，继承 LangChain 的 LLM 类自定义一个 InternLM LLM 子类，从而实现将 InternLM 接入到 LangChain 框架中。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">from langchain.llms.base import LLM</span><br><span class="line">from typing import Any, List, Optional</span><br><span class="line">from langchain.callbacks.manager import CallbackManagerForLLMRun</span><br><span class="line">from transformers import AutoTokenizer, AutoModelForCausalLM</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">class InternLM_LLM(LLM):</span><br><span class="line">    # 基于本地 InternLM 自定义 LLM 类</span><br><span class="line">    tokenizer : AutoTokenizer = None</span><br><span class="line">    model: AutoModelForCausalLM = None</span><br><span class="line"></span><br><span class="line">    def __init__(self, model_path :str):</span><br><span class="line">        # model_path: InternLM 模型路径</span><br><span class="line">        # 从本地初始化模型</span><br><span class="line">        super().__init__()</span><br><span class="line">        print(&quot;正在从本地加载模型...&quot;)</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)</span><br><span class="line">        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(torch.bfloat16).cuda()</span><br><span class="line">        self.model = self.model.eval()</span><br><span class="line">        print(&quot;完成本地模型的加载&quot;)</span><br><span class="line"></span><br><span class="line">    def _call(self, prompt : str, stop: Optional[List[str]] = None,</span><br><span class="line">                run_manager: Optional[CallbackManagerForLLMRun] = None,</span><br><span class="line">                **kwargs: Any):</span><br><span class="line">        # 重写调用函数</span><br><span class="line">        system_prompt = &quot;&quot;&quot;You are an AI assistant whose name is InternLM (书生·浦语).</span><br><span class="line">        - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span><br><span class="line">        - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        </span><br><span class="line">        messages = [(system_prompt, &#x27;&#x27;)]</span><br><span class="line">        response, history = self.model.chat(self.tokenizer, prompt , history=messages)</span><br><span class="line">        return response</span><br><span class="line">        </span><br><span class="line">    @property</span><br><span class="line">    def _llm_type(self) -&gt; str:</span><br><span class="line">        return &quot;InternLM&quot;</span><br></pre></td></tr></table></figure>
<h4 id="构建检索问答链"><a href="#构建检索问答链" class="headerlink" title="构建检索问答链"></a>构建检索问答链</h4><ul>
<li>加载向量数据库</li>
<li>实例化自定义 LLM 与 Prompt Template</li>
<li>构建检索问答链</li>
<li>三步的代码合在下面Web Demo里了</li>
</ul>
<h4 id="部署-Web-Demo"><a href="#部署-Web-Demo" class="headerlink" title="部署 Web Demo"></a>部署 Web Demo</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">from langchain.vectorstores import Chroma</span><br><span class="line">from langchain.embeddings.huggingface import HuggingFaceEmbeddings</span><br><span class="line">import os</span><br><span class="line">from InternLM_LLM import InternLM_LLM</span><br><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line">from langchain.chains import RetrievalQA</span><br><span class="line"></span><br><span class="line">def load_chain():</span><br><span class="line">    # 加载问答链</span><br><span class="line">    # 定义 Embeddings</span><br><span class="line">    embeddings = HuggingFaceEmbeddings(model_name=&quot;/root/model/sentence-transformer&quot;)</span><br><span class="line"></span><br><span class="line">    # 向量数据库持久化路径</span><br><span class="line">    persist_directory = &#x27;data_base/vector_db/chroma&#x27;</span><br><span class="line"></span><br><span class="line">    # 加载数据库</span><br><span class="line">    vectordb = Chroma(</span><br><span class="line">        persist_directory=persist_directory,  # 允许我们将persist_directory目录保存到磁盘上</span><br><span class="line">        embedding_function=embeddings</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 加载自定义 LLM</span><br><span class="line">    llm = InternLM_LLM(model_path = &quot;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;)</span><br><span class="line"></span><br><span class="line">    # 定义一个 Prompt Template</span><br><span class="line">    template = &quot;&quot;&quot;使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答</span><br><span class="line">    案。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。</span><br><span class="line">    &#123;context&#125;</span><br><span class="line">    问题: &#123;question&#125;</span><br><span class="line">    有用的回答:&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[&quot;context&quot;,&quot;question&quot;],template=template)</span><br><span class="line"></span><br><span class="line">    # 运行 chain</span><br><span class="line">    qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=True,chain_type_kwargs=&#123;&quot;prompt&quot;:QA_CHAIN_PROMPT&#125;)</span><br><span class="line">    </span><br><span class="line">    return qa_chain</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Model_center():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    存储检索问答链的对象 </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 构造函数，加载检索问答链</span><br><span class="line">        self.chain = load_chain()</span><br><span class="line"></span><br><span class="line">    def qa_chain_self_answer(self, question: str, chat_history: list = []):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        调用问答链进行回答</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if question == None or len(question) &lt; 1:</span><br><span class="line">            return &quot;&quot;, chat_history</span><br><span class="line">        try:</span><br><span class="line">            chat_history.append(</span><br><span class="line">                (question, self.chain(&#123;&quot;query&quot;: question&#125;)[&quot;result&quot;]))</span><br><span class="line">            # 将问答结果直接附加到问答历史中，Gradio 会将其展示出来</span><br><span class="line">            return &quot;&quot;, chat_history</span><br><span class="line">        except Exception as e:</span><br><span class="line">            return e, chat_history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    import gradio as gr</span><br><span class="line">    # 实例化核心功能对象</span><br><span class="line">    model_center = Model_center()</span><br><span class="line">    # 创建一个 Web 界面</span><br><span class="line">    block = gr.Blocks()</span><br><span class="line">    with block as demo:</span><br><span class="line">        with gr.Row(equal_height=True):   </span><br><span class="line">            with gr.Column(scale=15):</span><br><span class="line">                # 展示的页面标题</span><br><span class="line">                gr.Markdown(&quot;&quot;&quot;&lt;h1&gt;&lt;center&gt;InternLM&lt;/center&gt;&lt;/h1&gt;</span><br><span class="line">                    &lt;center&gt;书生浦语&lt;/center&gt;</span><br><span class="line">                    &quot;&quot;&quot;)</span><br><span class="line"></span><br><span class="line">        with gr.Row():</span><br><span class="line">            with gr.Column(scale=4):</span><br><span class="line">                # 创建一个聊天机器人对象</span><br><span class="line">                chatbot = gr.Chatbot(height=450, show_copy_button=True)</span><br><span class="line">                # 创建一个文本框组件，用于输入 prompt。</span><br><span class="line">                msg = gr.Textbox(label=&quot;Prompt/问题&quot;)</span><br><span class="line"></span><br><span class="line">                with gr.Row():</span><br><span class="line">                    # 创建提交按钮。</span><br><span class="line">                    db_wo_his_btn = gr.Button(&quot;Chat&quot;)</span><br><span class="line">                with gr.Row():</span><br><span class="line">                    # 创建一个清除按钮，用于清除聊天机器人组件的内容。</span><br><span class="line">                    clear = gr.ClearButton(</span><br><span class="line">                        components=[chatbot], value=&quot;Clear console&quot;)</span><br><span class="line">                    </span><br><span class="line">            # 设置按钮的点击事件。当点击时，调用上面定义的 qa_chain_self_answer 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。</span><br><span class="line">            db_wo_his_btn.click(model_center.qa_chain_self_answer, inputs=[</span><br><span class="line">                                msg, chatbot], outputs=[msg, chatbot])</span><br><span class="line"></span><br><span class="line">        gr.Markdown(&quot;&quot;&quot;提醒：&lt;br&gt;</span><br><span class="line">        1. 初始化数据库时间可能较长，请耐心等待。</span><br><span class="line">        2. 使用中如果出现异常，将会在文本输入框进行展示，请不要惊慌。 &lt;br&gt;</span><br><span class="line">        &quot;&quot;&quot;)</span><br><span class="line">    gr.close_all()</span><br><span class="line">    # 直接启动</span><br><span class="line">    demo.launch()</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/aqizhoua/picx-images-hosting@master/20240114/7.4jhwdub0jde0.webp" alt="6"></p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/poster/8e7b9ade.html"
      title="lesson3作业"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        lesson3作业
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/poster/5b298b12.html"
      title="卡尔曼滤波器"
     >

    <p class="title-text">
      
        卡尔曼滤波器
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>


  
  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <div id="comment-card" class="comment-card">
    <div class="main-title-bar">
      <div class="main-title-dot"></div>
      <div class="main-title">Comments </div>
    </div>
    <div id="vcomments"></div>
  </div>
  <script>
      new Valine({"enable":true,"appId":null,"appKey":null,"placeholder":"Just go go","pageSize":10,"highlight":true,"serverURLs":null,"el":"#vcomments"});
  </script>



    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2024 aqizhoua<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
