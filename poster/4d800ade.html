<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="false" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>lesson5_LMDeploy 的量化和部署 | aqizhoua</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="Lecture5 LMDeploy大模型量化部署实践 在Lecture5中，我们首先深入了解了大模型的特点，包括其庞大的规模以及在部署过程中面临的挑战。接着，我们详细学习了LMDeploy框架的三个核心功能，分别是轻量化、推理引擎TurboMind和服务。此外，我们还进行了实际的安装、部署和量化操作，以加深对这些概念的理解和应用。 课前的一些问题~ 什么是LMDeploy？ LMDeploy是大语">
<meta property="og:type" content="article">
<meta property="og:title" content="lesson5_LMDeploy 的量化和部署">
<meta property="og:url" content="https://aqizhoua.github.io/poster/4d800ade.html">
<meta property="og:site_name" content="aqizhoua">
<meta property="og:description" content="Lecture5 LMDeploy大模型量化部署实践 在Lecture5中，我们首先深入了解了大模型的特点，包括其庞大的规模以及在部署过程中面临的挑战。接着，我们详细学习了LMDeploy框架的三个核心功能，分别是轻量化、推理引擎TurboMind和服务。此外，我们还进行了实际的安装、部署和量化操作，以加深对这些概念的理解和应用。 课前的一些问题~ 什么是LMDeploy？ LMDeploy是大语">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fc1cf07fe-3e21-44f2-8ee6-104386a06d40%2FUntitled.png?table=block&amp;id=884b135c-3b9a-4d29-9042-6af382c72d7c&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F3b4387c2-6458-435a-9d48-06c45a6656c0%2FUntitled.png?table=block&amp;id=5db10ae6-edd7-4943-ba3d-99ab7ac20d80&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fe0861984-1bad-4a1e-8524-bb5bab38290c%2FUntitled.png?table=block&amp;id=76cb4ec6-ab9d-4814-b7a1-2e4c53911d6f&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Ffef27b99-71af-4aa4-aa07-6ef022009f15%2FUntitled.png?table=block&amp;id=87ade43a-2ac6-4e53-b8ef-3f2380dad878&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F80f99e20-d422-457d-a105-5afa9e6cc196%2FUntitled.png?table=block&amp;id=aba3b1ff-0ab9-4832-bb89-012819674fdc&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F9bb06b52-10e5-4472-80e5-c1461e33cd26%2FUntitled.png?table=block&amp;id=360a72ff-0bea-423f-b4e1-8f2ba5bb8074&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=1240&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F37848500-f81b-416b-aca2-d9e18dd9f909%2FUntitled.png?table=block&amp;id=77afda24-a04f-4c08-b290-74c29c7368da&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fdcb86792-572a-4c54-97b7-03e434bd7de9%2FUntitled.png?table=block&amp;id=1b0e92f5-e79c-4f5c-9fcc-b4e48040f9ae&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fa776381f-3e4c-4b59-ab44-dffc43af8255%2FUntitled.png?table=block&amp;id=8002c102-c63e-4684-aad0-7bbd97a01224&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=580&amp;userId=&amp;cache=v2">
<meta property="og:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F6a789c6a-66d4-4c6f-8162-8fa603cd1f75%2FUntitled.png?table=block&amp;id=e22c406b-ffa1-4919-af01-c0396a504faa&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=380&amp;userId=&amp;cache=v2">
<meta property="article:published_time" content="2024-02-21T10:09:32.000Z">
<meta property="article:modified_time" content="2024-02-21T11:20:18.083Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="书生浦语大模型实战营">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fc1cf07fe-3e21-44f2-8ee6-104386a06d40%2FUntitled.png?table=block&amp;id=884b135c-3b9a-4d29-9042-6af382c72d7c&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  
   
  <div id="main-grid" class="shadow   ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>aqizhoua </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/fanren">Fanren</a>
    
      <a class="main-nav-link" href="/geek">geek</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/fanren">Fanren</a>
    
      <a class="nav-dropdown-link" href="/geek">geek</a>
    
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/sleep.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">aqizhoua </div>
      <div class="dot"></div>
      <div class="subtitle">人生是旷野 </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/aqizhoua" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Categories</h3>
      <div class="category-box"></div>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Tags</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/MOT/" rel="tag">MOT</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/hexo/" rel="tag">hexo</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/smtp/" rel="tag">smtp</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5/" rel="tag">书生浦语大模型实战营</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/" rel="tag">数学知识</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" rel="tag">每日一题</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Archives</h3>
      
      
        <a class="archive-link" href="/archives/2024/02 ">
          February 2024 
          <div class="archive-count">7 </div>
        </a>
      
        <a class="archive-link" href="/archives/2024/01 ">
          January 2024 
          <div class="archive-count">14 </div>
        </a>
      
        <a class="archive-link" href="/archives/2023/11 ">
          November 2023 
          <div class="archive-count">5 </div>
        </a>
      
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Recent Posts</h3>
      <ul>
        
          <a class="recent-link" href="/poster/edabafe4.html" title="lesson5作业" >
            <div class="recent-link-text">
              lesson5作业
            </div>
          </a>
        
          <a class="recent-link" href="/poster/dc43b579.html" title="lesson6作业" >
            <div class="recent-link-text">
              lesson6作业
            </div>
          </a>
        
          <a class="recent-link" href="/poster/fa4489c9.html" title="lesson6_OpenCompass大模型测评" >
            <div class="recent-link-text">
              lesson6_OpenCompass大模型测评
            </div>
          </a>
        
          <a class="recent-link" href="/poster/4d800ade.html" title="lesson5_LMDeploy 的量化和部署" >
            <div class="recent-link-text">
              lesson5_LMDeploy 的量化和部署
            </div>
          </a>
        
          <a class="recent-link" href="/poster/4bdca450.html" title="lesson4作业" >
            <div class="recent-link-text">
              lesson4作业
            </div>
          </a>
        
      </ul>
    </div>
  </div>

    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-lesson5-LMDeploy-的量化和部署" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        lesson5_LMDeploy 的量化和部署
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-02-21T10:09:32.000Z" itemprop="datePublished">2024-02-21</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            6.3k words 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5/" rel="tag">书生浦语大模型实战营</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h1 id="Lecture5-LMDeploy大模型量化部署实践"><a href="#Lecture5-LMDeploy大模型量化部署实践" class="headerlink" title="Lecture5 LMDeploy大模型量化部署实践"></a>Lecture5 LMDeploy大模型量化部署实践</h1><p> 在<a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/blob/main/lmdeploy/lmdeploy.md">Lecture5</a>中，我们首先深入了解了大模型的特点，包括其庞大的规模以及在部署过程中面临的挑战。接着，我们详细学习了<code>LMDeploy</code>框架的三个核心功能，分别是轻量化、推理引擎<code>TurboMind</code>和服务。此外，我们还进行了实际的安装、部署和量化操作，以加深对这些概念的理解和应用。</p>
<p>课前的一些问题~</p>
<p>什么是<code>LMDeploy</code>？</p>
<p><code>LMDeploy</code>是大语言模型在英伟达设备上部署的全流程方案。包括模型轻量化、推理和服务。（还没有涉及到移动端），<a target="_blank" rel="noopener" href="https://github.com/internLM/lmdeploy">项目地址</a></p>
<p>什么是量化？为什么要量化？</p>
<p>显存降低了，存储空间增多了，同样的设备下能够容纳更多，提高速度。</p>
<h2 id="一、大模型部署背景"><a href="#一、大模型部署背景" class="headerlink" title="一、大模型部署背景"></a>一、大模型部署背景</h2><p> <strong>什么是模型部署？</strong></p>
<ul>
<li><strong>定义：</strong>将训练好的模型，在特定的硬软件环境中启动运行起来，能够接收一定的输入并返回一定的输出。</li>
<li><p>需要考虑<strong>模型的压缩和硬件的加速</strong></p>
<p><strong>大模型特点：</strong></p>
</li>
<li><p><strong>消耗显存内存大</strong>：大模型的参数量很大；需要token by token，占用显存很大</p>
</li>
<li><strong>动态shape：</strong>请求数不确定，token逐个生成且数量不定</li>
<li><strong>结构简单：</strong>transformer结构，大部分是decoder-only</li>
</ul>
<p><strong>大模型部署的挑战：</strong></p>
<ul>
<li><strong>设备挑战</strong>：巨大的储存、如何部署</li>
<li><strong>推理</strong>：怎么加速token生成速度；怎么有效管理内存；怎么把batch文本的随机数量，让其推理不间断</li>
<li><strong>服务</strong>：怎么提升整体吞吐量；对个体用户的响应时间</li>
</ul>
<p><strong>大模型部署方案</strong></p>
<ul>
<li>技术点：模型并行、低比特量化、page attention、transformer计算和访存优化、continuous batch<strong>？？？</strong></li>
<li><strong>方案</strong> 云端方案：deepspeed、vllm、Imdeploy 移动端：llama.cpp、mlc-llm</li>
</ul>
<h2 id="二、LMDeploy简介"><a href="#二、LMDeploy简介" class="headerlink" title="二、LMDeploy简介"></a>二、LMDeploy简介</h2><p>部署有很多成熟的框架，那LMDeploy是什么呢？</p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fc1cf07fe-3e21-44f2-8ee6-104386a06d40%2FUntitled.png?table=block&amp;id=884b135c-3b9a-4d29-9042-6af382c72d7c&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2" alt="Untitled"></p>
<p><strong>① LMDeploy的内容：</strong></p>
<p>1、轻量化</p>
<p>2、推理引擎：turbomind（<strong>创新点</strong>、支持pytorch）</p>
<p>3、服务：api service、gradio、triton inference server</p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F3b4387c2-6458-435a-9d48-06c45a6656c0%2FUntitled.png?table=block&amp;id=5db10ae6-edd7-4943-ba3d-99ab7ac20d80&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2" alt="Untitled"></p>
<p><strong>② 怎么做weight only的量化？</strong></p>
<ul>
<li><p>AWQ算法：在张量/矩阵进行计算的过程中，不量化最重要的少部分参数…</p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fe0861984-1bad-4a1e-8524-bb5bab38290c%2FUntitled.png?table=block&amp;id=76cb4ec6-ab9d-4814-b7a1-2e4c53911d6f&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2" alt="Untitled"></p>
</li>
</ul>
<p><strong>③ 推理引擎TurboMind</strong></p>
<ul>
<li><p>持续批处理: 请求可以及时加入batch推理，Batch中已经完成的请求及时退出。是动态的，保证不管怎么进来，service都不用等到整个batch结束。</p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Ffef27b99-71af-4aa4-aa07-6ef022009f15%2FUntitled.png?table=block&amp;id=87ade43a-2ac6-4e53-b8ef-3f2380dad878&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2" alt="Untitled"></p>
</li>
<li><p>有状态的推理：对话 token 被缓存在推理侧。用户侧请求无需带上历史对话记录。</p>
</li>
<li><p>高性能算子</p>
</li>
<li><p>Blocked k/v cache</p>
</li>
</ul>
<p><strong>④ 服务</strong></p>
<p>可以通过一个简单的命令运行一个api服务</p>
<p>lmdeploy server api_server InterLM/internlm-chat-7b —model-name internlm-chat-6b —server-port 8080</p>
<h2 id="三、LMDeploy实战"><a href="#三、LMDeploy实战" class="headerlink" title="三、LMDeploy实战"></a>三、LMDeploy实战</h2><ul>
<li><p><strong>环境配置</strong></p>
<ul>
<li><p>设置开发机</p>
<p>在 <a target="_blank" rel="noopener" href="https://studio.intern-ai.org.cn/console/instance">InternStudio</a> 平台中选择 A100(1/4) 的配置，打开刚刚租用服务器的 <code>进入VScode</code>，并且打开其中的终端开始环境配置。</p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F80f99e20-d422-457d-a105-5afa9e6cc196%2FUntitled.png?table=block&amp;id=aba3b1ff-0ab9-4832-bb89-012819674fdc&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2" alt="Untitled"></p>
<p>进入 <code>VScode</code>，并在终端输入 <code>bash</code> 命令，进入 <code>conda</code> 环境。</p>
<p>进入 <code>conda</code> 环境之后，使用以下命令从本地一个已有的 <code>pytorch 2.0.1</code> 的环境创建需要的环境</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bash</span><br><span class="line">conda create -n Lmdeploy --<span class="built_in">clone</span> /share/conda_envs/internlm-base</span><br></pre></td></tr></table></figure>
<p>然后使用以下命令激活环境</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate Lmdeploy</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载<code>imdeploy</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install packaging</span><br><span class="line">pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</span><br><span class="line">pip install <span class="string">&#x27;lmdeploy[all]==v0.1.0&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>服务部署</strong></p>
<ul>
<li><p>原理介绍CS架构：左边service（模型推理各种优化策略+暴露服务放在API），右边client（终端用户的使用）。</p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F9bb06b52-10e5-4472-80e5-c1461e33cd26%2FUntitled.png?table=block&amp;id=360a72ff-0bea-423f-b4e1-8f2ba5bb8074&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=1240&amp;userId=&amp;cache=v2" alt="Untitled"></p>
</li>
</ul>
<ol>
<li><p>模型转换 ① 在线转换：hf通过imdeploy量化；hf上面其他LM模型 ② 离线转换：指定模型名称、模型路径，转换为<code>turbomind</code>需要的格式，如下图</p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F37848500-f81b-416b-aca2-d9e18dd9f909%2FUntitled.png?table=block&amp;id=77afda24-a04f-4c08-b290-74c29c7368da&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2" alt="Untitled"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/</span><br><span class="line"></span><br><span class="line">lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/ --model-name internlm-chat-7b</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>TurboMind 推理+命令行本地对话</strong> <strong>此时是跳过 API Server 直接调用 TurboMind。Server 就是本地跑起来的模型（TurboMind），命令行启用Client可以看作是前端。</strong></p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fdcb86792-572a-4c54-97b7-03e434bd7de9%2FUntitled.png?table=block&amp;id=1b0e92f5-e79c-4f5c-9fcc-b4e48040f9ae&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=2000&amp;userId=&amp;cache=v2" alt="Untitled"></p>
</li>
<li><p><strong>TurboMind推理+API服务</strong> <strong>运用 lmdepoy 进行服务化，使用Turbomind的API Server 可以提供对外的 API 服务来进行服务。</strong> 下面的参数中 <code>server_name</code> 和 <code>server_port</code> 分别表示服务地址和端口，<code>tp</code> 参数我们之前已经提到过了，表示 Tensor 并行。还剩下一个 <code>instance_num</code> 参数，表示实例数，可以理解成 Batch 的大小。执行后如下图所示。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ApiServer+Turbomind   api_server =&gt; AsyncEngine =&gt; TurboMind</span></span><br><span class="line">lmdeploy serve api_server ./workspace \\</span><br><span class="line">	--server_name 0.0.0.0 \\</span><br><span class="line">	--server_port 23333 \\</span><br><span class="line">	--instance_num 64 \\</span><br><span class="line">	--tp 1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li><p><strong>TurboMind 推理作为后端：</strong>Gradio 也可以直接和 TurboMind 连接</p>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2Fa776381f-3e4c-4b59-ab44-dffc43af8255%2FUntitled.png?table=block&amp;id=8002c102-c63e-4684-aad0-7bbd97a01224&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=580&amp;userId=&amp;cache=v2" alt="Untitled"></p>
</li>
<li><p><strong>TurboMind 服务作为后端：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Gradio+ApiServer。必须先开启 Server，此时 Gradio 为 Client</span><br><span class="line">lmdeploy serve gradio &lt;http://0.0.0.0:23333&gt; \\</span><br><span class="line">	--server_name 0.0.0.0 \\</span><br><span class="line">	--server_port 6006 \\</span><br><span class="line">	--restful_api True</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id><a href="#" class="headerlink" title=" "></a> </h1></li>
<li><p><strong>分场景应用API 服务和 Client</strong></p>
<ul>
<li>我想对外提供类似 OpenAI 那样的 HTTP 接口服务。推荐使用 TurboMind推理 + API 服务。</li>
<li>我想做一个演示 Demo，Gradio 无疑是比 Local Chat 更友好的。推荐使用 TurboMind 推理作为后端的Gradio进行演示。</li>
<li>我想直接在自己的 Python 项目中使用大模型功能。推荐使用 TurboMind推理 + Python。</li>
<li>我想在自己的其他非 Python 项目中使用大模型功能。推荐直接通过 HTTP 接口调用服务。也就是用本列表第一条先启动一个 HTTP API 服务，然后在项目中直接调用接口。</li>
<li>我的项目是 C++ 写的，为什么不能直接用 TurboMind 的 C++ 接口？！必须可以！大佬可以右上角叉掉这个窗口啦。</li>
</ul>
</li>
<li><p><strong>模型配置和参数调整</strong></p>
<p>这里主要介绍三个可能需要调整的参数。</p>
<ul>
<li>KV int8 开关：<ul>
<li>对应参数为 <code>quant_policy</code>，默认值为 0，表示不使用 KV Cache，如果需要开启，则将该参数设置为 4。</li>
<li>KV Cache 是对序列生成过程中的 K 和 V 进行量化，用以节省显存。我们下一部分会介绍具体的量化过程。</li>
<li>当显存不足，或序列比较长时，建议打开此开关。</li>
</ul>
</li>
<li>外推能力开关：<ul>
<li>对应参数为 <code>rope_scaling_factor</code>，默认值为 0.0，表示不具备外推能力，设置为 1.0，可以开启 RoPE 的 Dynamic NTK 功能，支持长文本推理。另外，<code>use_logn_attn</code> 参数表示 Attention 缩放，默认值为 0，如果要开启，可以将其改为 1。</li>
<li>外推能力是指推理时上下文的长度超过训练时的最大长度时模型生成的能力。如果没有外推能力，当推理时上下文长度超过训练时的最大长度，效果会急剧下降。相反，则下降不那么明显，当然如果超出太多，效果也会下降的厉害。</li>
<li>当推理文本非常长（明显超过了训练时的最大长度）时，建议开启外推能力。</li>
</ul>
</li>
<li>批处理大小：<ul>
<li>对应参数为 <code>max_batch_size</code>，默认为 64，也就是我们在 API Server 启动时的 <code>instance_num</code> 参数。</li>
<li>该参数值越大，吞度量越大（同时接受的请求数），但也会占用更多显存。</li>
<li>建议根据请求量和最大的上下文长度，按实际情况调整。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="四、模型量化"><a href="#四、模型量化" class="headerlink" title="四、模型量化"></a>四、模型量化</h2><p>首先我们需要明白一点，服务部署和量化是没有直接关联的，量化的最主要目的是降低显存占用，主要包括两方面的显存：模型参数和中间过程计算结果。前者对应《W4A16 量化》，后者对应《 KV Cache 量化》。</p>
<h3 id="W4A16-量化"><a href="#W4A16-量化" class="headerlink" title="W4A16 量化"></a>W4A16 量化</h3><p>W4A16中的A是指Activation，保持FP16，只对参数进行 4bit 量化。使用过程也可以看作是三步。</p>
<ul>
<li><p>量化步骤</p>
<p>第一步：同KV Cache量化，不再赘述。</p>
<p>第二步：量化权重模型。利用第一步得到的统计值对参数进行量化，具体又包括两小步：</p>
<ul>
<li>缩放参数。主要是性能上的考虑（回顾 PPT）。</li>
<li>整体量化。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二步的执行命令如下：</span></span><br><span class="line"><span class="comment"># 量化权重模型</span></span><br><span class="line">lmdeploy lite auto_awq \\</span><br><span class="line">  --model  /root/share/temp/model_repos/internlm-chat-7b/ \\</span><br><span class="line">  --w_bits 4 \\</span><br><span class="line">  --w_group_size 128 \\</span><br><span class="line">  --work_dir ./quant_output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 命令中 w_bits 表示量化的位数，w_group_size 表示量化分组统计的尺寸，</span></span><br><span class="line"><span class="comment"># work_dir 是量化后模型输出的位置。</span></span><br><span class="line"><span class="comment"># 这里需要特别说明的是，因为没有 torch.int4，所以实际存储时，8个 4bit 权重会被打包到一个 int32 值中。所以，如果你把这部分量化后的参数加载进来就会发现它们是 int32 类型的。</span></span><br></pre></td></tr></table></figure>
<p>最后一步：转换成 TurboMind 格式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 转换模型的layout，存放在默认路径 ./workspace 下</span><br><span class="line">lmdeploy convert  internlm-chat-7b ./quant_output \\</span><br><span class="line">    --model-format awq \\</span><br><span class="line">    --group-size 128</span><br></pre></td></tr></table></figure>
<p>这个 <code>group-size</code> 就是上一步的那个 <code>w_group_size</code>。如果不想和之前的 <code>workspace</code> 重复，可以指定输出目录：<code>--dst_path</code>，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy convert  internlm-chat-7b ./quant_output \\</span><br><span class="line">    --model-format awq \\</span><br><span class="line">    --group-size 128 \\</span><br><span class="line">    --dst_path ./workspace_quant</span><br></pre></td></tr></table></figure>
<p>接下来和上一节一样，可以正常运行前面的各种服务了，不过咱们现在用的是量化后的模型。</p>
</li>
</ul>
<h3 id="KV-Cache-量化"><a href="#KV-Cache-量化" class="headerlink" title="KV Cache 量化"></a>KV Cache 量化</h3><p>KV Cache 量化是将已经生成序列的 KV 变成 Int8</p>
<ul>
<li><p>量化步骤</p>
<p>第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算 minmax</span></span><br><span class="line">lmdeploy lite calibrate \\</span><br><span class="line">--model  /root/share/temp/model_repos/internlm-chat-7b/ \\</span><br><span class="line">--calib_dataset <span class="string">&quot;c4&quot;</span> \\</span><br><span class="line">--calib_samples 128 \\</span><br><span class="line">--calib_seqlen 2048 \\</span><br><span class="line">--work_dir ./quant_output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这个命令行中，会选择 128 条输入样本，每条样本长度为 2048，</span></span><br><span class="line"><span class="comment"># 数据集选择 C4，输入模型后就会得到上面的各种统计值。</span></span><br><span class="line"><span class="comment"># 值得说明的是，如果显存不足，可以适当调小 samples 的数量或 sample 的长度。</span></span><br></pre></td></tr></table></figure>
<ul>
<li>对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 <code>(num_heads, head_dim)</code> 的矩阵。这里的统计结果将用于本小节的 KV Cache。</li>
<li>对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 <code>(hidden_dim, )</code> 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同。这里的统计结果用于下个小节的模型参数量化，主要用在缩放环节（回顾PPT内容）。</li>
</ul>
<p>第二步：通过 minmax 获取量化参数。主要就是利用下面这个公式，获取每一层的 K V 中心值（zp）和缩放值（scale）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zp = (min+max) / 2</span><br><span class="line">scale = (max-min) / 255</span><br><span class="line">quant: q = round( (f-zp) / scale)</span><br><span class="line">dequant: f = q * scale + zp</span><br></pre></td></tr></table></figure>
<p>有这两个值就可以进行量化和解量化操作了。具体来说，就是对历史的 K 和 V 存储 quant 后的值，使用时在 dequant。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二步的执行命令如下：</span></span><br><span class="line"><span class="comment"># 通过 minmax 获取量化参数</span></span><br><span class="line">lmdeploy lite kv_qparams \\</span><br><span class="line">  --work_dir ./quant_output  \\</span><br><span class="line">  --turbomind_dir workspace/triton_models/weights/ \\</span><br><span class="line">  --kv_sym False \\</span><br><span class="line">  --num_tp 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这个命令中，num_tp 表示 Tensor 的并行数。每一层的中心值和缩放值会存储到 workspace 的参数目录中以便后续使用。</span></span><br><span class="line"><span class="comment"># kv_sym 为 True 时会使用另一种（对称）量化方法，它用到了第一步存储的绝对值最大值，而不是最大值和最小值。</span></span><br></pre></td></tr></table></figure>
<p>第三步：修改配置。也就是修改 <code>weights/config.ini</code> 文件，这个我们在《2.6.2 模型配置实践》中已经提到过了（KV int8 开关），只需要把 <code>quant_policy</code> 改为 4 即可。</p>
<p>这一步需要额外说明的是，如果用的是 TurboMind1.0，还需要修改参数 <code>use_context_fmha</code>，将其改为 0。</p>
<p>接下来就可以正常运行前面的各种服务了，只不过咱们现在可是用上了 KV Cache 量化，能更省（运行时）显存了。</p>
</li>
</ul>
<h3 id="量化流程"><a href="#量化流程" class="headerlink" title="量化流程"></a>量化流程</h3><p>建议是：在各种配置下尝试，看效果能否满足需要。这一般需要在自己的数据集上进行测试。具体步骤如下。</p>
<ul>
<li>Step1：优先尝试正常（非量化）版本，评估效果。<ul>
<li>如果效果不行，需要尝试更大参数模型或者微调。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step2：尝试正常版本+KV Cache 量化，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step3：尝试量化版本，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step4：尝试量化版本+ KV Cache 量化，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，使用方案。</li>
</ul>
</li>
</ul>
<p><img src="https://broadleaf-gemini-274.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fc79bd034-8c99-48fa-85d9-e28d53b85138%2F6a789c6a-66d4-4c6f-8162-8fa603cd1f75%2FUntitled.png?table=block&amp;id=e22c406b-ffa1-4919-af01-c0396a504faa&amp;spaceId=c79bd034-8c99-48fa-85d9-e28d53b85138&amp;width=380&amp;userId=&amp;cache=v2" alt="Untitled"></p>
<p><strong>SUMMARY: </strong></p>
<p><strong>部署：</strong>前面我们实战中的大模型，其实已经进行过部署实战了，就是让本地（或者在<code>InternStudio</code>上）跑起来一个<code>chat</code>模型。一般采用<code>streamlit</code>或者<code>gradio</code>的方式给一个页面，或者命令行方式启动。本节课具体介绍了模型部署涉及到的性能和效率的要求，以及考虑模型压缩，硬件加速等环节所作的工作。然后介绍了大模型的不同部署方案，最后具体介绍了<code>**LMDeploy**</code>这个全流程部署方案，及其部署策略优势和内容，并开展了基于<code>**LMDeploy**</code>的实战练习。</p>
<p> <strong>量化：</strong>量化的最主要目的是降低显存占用，量化在降低显存的同时，一般还能带来性能的提升，因为更小精度的浮点数要比高精度的浮点数计算效率高，而整型要比浮点数高很多，KV Cache 量化就是如此。此外量化模型和 KV Cache 量化也可以一起使用，以达到最大限度节省显存。</p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/poster/fa4489c9.html"
      title="lesson6_OpenCompass大模型测评"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        lesson6_OpenCompass大模型测评
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/poster/4bdca450.html"
      title="lesson4作业"
     >

    <p class="title-text">
      
        lesson4作业
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>


  
  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <div id="comment-card" class="comment-card">
    <div class="main-title-bar">
      <div class="main-title-dot"></div>
      <div class="main-title">Comments </div>
    </div>
    <div id="vcomments"></div>
  </div>
  <script>
      new Valine({"enable":true,"appId":null,"appKey":null,"placeholder":"Just go go","pageSize":10,"highlight":true,"serverURLs":null,"el":"#vcomments"});
  </script>



    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2024 aqizhoua<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
