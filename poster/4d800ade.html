<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="false" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>lesson5_LMDeploy 的量化和部署 | aqizhoua</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="LMDeploy 的量化和部署   1 环境配置 2 服务部署 2.1 模型转换 2.1.1 在线转换 2.1.2 离线转换   2.2  TurboMind 推理+命令行本地对话 2.3 TurboMind推理+API服务 2.4 网页 Demo 演示 2.4.1 TurboMind 服务作为后端 2.4.2 TurboMind 推理作为后端   2.5 TurboMind 推理 + Pytho">
<meta property="og:type" content="article">
<meta property="og:title" content="lesson5_LMDeploy 的量化和部署">
<meta property="og:url" content="https://aqizhoua.github.io/poster/4d800ade.html">
<meta property="og:site_name" content="aqizhoua">
<meta property="og:description" content="LMDeploy 的量化和部署   1 环境配置 2 服务部署 2.1 模型转换 2.1.1 在线转换 2.1.2 离线转换   2.2  TurboMind 推理+命令行本地对话 2.3 TurboMind推理+API服务 2.4 网页 Demo 演示 2.4.1 TurboMind 服务作为后端 2.4.2 TurboMind 推理作为后端   2.5 TurboMind 推理 + Pytho">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://aqizhoua.github.io/poster/0.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/add0.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/add1.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/1.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/2.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/add3.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/lmdeploy.drawio.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/4.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/5.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/6.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/7.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/8.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/11.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/12.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/13.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/20.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/16.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/17.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/19.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/add4.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/quant.drawio.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/3.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/14.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/15.png">
<meta property="og:image" content="https://aqizhoua.github.io/poster/18.png">
<meta property="article:published_time" content="2024-02-21T10:09:32.000Z">
<meta property="article:modified_time" content="2024-02-21T11:01:19.128Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="书生浦语大模型实战营">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://aqizhoua.github.io/poster/0.png">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  
   
  <div id="main-grid" class="shadow   ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>aqizhoua </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/fanren">Fanren</a>
    
      <a class="main-nav-link" href="/geek">geek</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/fanren">Fanren</a>
    
      <a class="nav-dropdown-link" href="/geek">geek</a>
    
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/sleep.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">aqizhoua </div>
      <div class="dot"></div>
      <div class="subtitle">人生是旷野 </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/aqizhoua" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Categories</h3>
      <div class="category-box"></div>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Tags</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/MOT/" rel="tag">MOT</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/hexo/" rel="tag">hexo</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/smtp/" rel="tag">smtp</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5/" rel="tag">书生浦语大模型实战营</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/" rel="tag">数学知识</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" rel="tag">每日一题</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Archives</h3>
      
      
        <a class="archive-link" href="/archives/2024/02 ">
          February 2024 
          <div class="archive-count">6 </div>
        </a>
      
        <a class="archive-link" href="/archives/2024/01 ">
          January 2024 
          <div class="archive-count">14 </div>
        </a>
      
        <a class="archive-link" href="/archives/2023/11 ">
          November 2023 
          <div class="archive-count">5 </div>
        </a>
      
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Recent Posts</h3>
      <ul>
        
          <a class="recent-link" href="/poster/dc43b579.html" title="lesson6作业" >
            <div class="recent-link-text">
              lesson6作业
            </div>
          </a>
        
          <a class="recent-link" href="/poster/fa4489c9.html" title="lesson6_OpenCompass大模型测评" >
            <div class="recent-link-text">
              lesson6_OpenCompass大模型测评
            </div>
          </a>
        
          <a class="recent-link" href="/poster/4d800ade.html" title="lesson5_LMDeploy 的量化和部署" >
            <div class="recent-link-text">
              lesson5_LMDeploy 的量化和部署
            </div>
          </a>
        
          <a class="recent-link" href="/poster/4bdca450.html" title="lesson4作业" >
            <div class="recent-link-text">
              lesson4作业
            </div>
          </a>
        
          <a class="recent-link" href="/poster/290cb30.html" title="25. K 个一组翻转链表" >
            <div class="recent-link-text">
              25. K 个一组翻转链表
            </div>
          </a>
        
      </ul>
    </div>
  </div>

    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-lesson5-LMDeploy-的量化和部署" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        lesson5_LMDeploy 的量化和部署
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-02-21T10:09:32.000Z" itemprop="datePublished">2024-02-21</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            17k words 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E8%90%A5/" rel="tag">书生浦语大模型实战营</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h1 id="LMDeploy-的量化和部署"><a href="#LMDeploy-的量化和部署" class="headerlink" title="LMDeploy 的量化和部署"></a>LMDeploy 的量化和部署</h1><!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
<ul>
<li><a href="#1-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">1 环境配置</a></li>
<li><a href="#2-%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2">2 服务部署</a><ul>
<li><a href="#21-%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2">2.1 模型转换</a><ul>
<li><a href="#211-%E5%9C%A8%E7%BA%BF%E8%BD%AC%E6%8D%A2">2.1.1 在线转换</a></li>
<li><a href="#212-%E7%A6%BB%E7%BA%BF%E8%BD%AC%E6%8D%A2">2.1.2 离线转换</a></li>
</ul>
</li>
<li><a href="#22--turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D">2.2  TurboMind 推理+命令行本地对话</a></li>
<li><a href="#23-turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1">2.3 TurboMind推理+API服务</a></li>
<li><a href="#24-%E7%BD%91%E9%A1%B5-demo-%E6%BC%94%E7%A4%BA">2.4 网页 Demo 演示</a><ul>
<li><a href="#241-turbomind-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF">2.4.1 TurboMind 服务作为后端</a></li>
<li><a href="#242-turbomind-%E6%8E%A8%E7%90%86%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF">2.4.2 TurboMind 推理作为后端</a></li>
</ul>
</li>
<li><a href="#25-turbomind-%E6%8E%A8%E7%90%86--python-%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90">2.5 TurboMind 推理 + Python 代码集成</a></li>
<li><a href="#26-%E8%BF%99%E4%B9%88%E5%A4%9A%E5%A4%B4%E7%A7%83%E6%9C%89%E6%B2%A1%E6%9C%89%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">2.6 这么多，头秃，有没有最佳实践</a><ul>
<li><a href="#261-%E6%96%B9%E6%A1%88%E5%AE%9E%E8%B7%B5">2.6.1 方案实践</a></li>
<li><a href="#262-%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E5%AE%9E%E8%B7%B5">2.6.2 模型配置实践</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96">3 模型量化</a><ul>
<li><a href="#31-kv-cache-%E9%87%8F%E5%8C%96">3.1 KV Cache 量化</a><ul>
<li><a href="#311-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4">3.1.1 量化步骤</a></li>
<li><a href="#312-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C">3.1.2 量化效果</a></li>
</ul>
</li>
<li><a href="#32-w4a16-%E9%87%8F%E5%8C%96">3.2 W4A16 量化</a><ul>
<li><a href="#321-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4">3.2.1 量化步骤</a></li>
<li><a href="#322-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C">3.2.2 量化效果</a></li>
</ul>
</li>
<li><a href="#33-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">3.3 最佳实践</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
<li><a href="#%E9%99%84%E5%BD%951tritonserver-%E4%BD%9C%E4%B8%BA%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E">附录1：TritonServer 作为推理引擎</a><ul>
<li><a href="#tritonserver%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">TritonServer环境配置</a></li>
<li><a href="#tritonserver%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1">TritonServer推理+API服务</a></li>
<li><a href="#tritonserver-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF">TritonServer 服务作为后端</a></li>
</ul>
</li>
</ul>
<!-- END doctoc generated TOC please keep comment here to allow auto update -->
<h2 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1 环境配置"></a>1 环境配置</h2><p>首先我们可以使用 <code>vgpu-smi</code> 查看显卡资源使用情况。</p>
<p><img src="0.png" alt></p>
<p>大家可以使用系统给的 vscode 进行后面的开发。分别点击下图「1」和「2」的位置，就会在下方显示终端。</p>
<p><img src="add0.png" alt></p>
<p>可以点击终端（TERMINAL）窗口右侧的「+」号创建新的终端窗口。大家可以新开一个窗口，执行下面的命令实时观察 GPU 资源的使用情况。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ watch vgpu-smi</span><br></pre></td></tr></table></figure>
<p>结果如下图所示，该窗口会实时检测 GPU 卡的使用情况。</p>
<p><img src="add1.png" alt></p>
<p>接下来我们切换到刚刚的终端（就是上图右边的那个「bash」，下面的「watch」就是监控的终端），创建部署和量化需要的环境。建议大家使用官方提供的环境，使用 conda 直接复制。</p>
<p>这里 <code>/share/conda_envs</code> 目录下的环境是官方未大家准备好的基础环境，因为该目录是共享只读的，而我们后面需要在此基础上安装新的软件包，所以需要复制到我们自己的 conda 环境（该环境下我们是可写的）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ conda create -n CONDA_ENV_NAME --<span class="built_in">clone</span> /share/conda_envs/internlm-base</span><br></pre></td></tr></table></figure>
<ul>
<li>如果clone操作过慢，可采用如下操作:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /root/share/install_conda_env_internlm_base.sh lmdeploy</span><br></pre></td></tr></table></figure>
<p>我们取 <code>CONDA_ENV_NAME</code> 为 <code>lmdeploy</code>，复制完成后，可以在本地查看环境。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ conda <span class="built_in">env</span> list</span><br></pre></td></tr></table></figure>
<p>结果如下所示。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conda environments:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">base                  *  /root/.conda</span><br><span class="line">lmdeploy                 /root/.conda/envs/lmdeploy</span><br></pre></td></tr></table></figure>
<p>然后激活环境。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ conda activate lmdeploy</span><br></pre></td></tr></table></figure>
<p>注意，环境激活后，左边会显示当前（也就是 <code>lmdeploy</code>）的环境名称，如下图所示。</p>
<p><img src="1.png" alt></p>
<p>可以进入Python检查一下 PyTorch 和 lmdeploy 的版本。由于 PyTorch 在官方提供的环境里，我们应该可以看到版本显示，而 lmdeploy 需要我们自己安装，此处应该会提示“没有这个包”，如下图所示。</p>
<p><img src="2.png" alt></p>
<p>lmdeploy 没有安装，我们接下来手动安装一下，建议安装最新的稳定版。<br>如果是在 InternStudio 开发环境，需要先运行下面的命令，否则会报错。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决 ModuleNotFoundError: No module named &#x27;packaging&#x27; 问题</span></span><br><span class="line">pip install packaging</span><br><span class="line"><span class="comment"># 使用 flash_attn 的预编译包解决安装过慢问题</span></span><br><span class="line">pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install <span class="string">&#x27;lmdeploy[all]==v0.1.0&#x27;</span></span><br></pre></td></tr></table></figure>
<p>由于默认安装的是 runtime 依赖包，但是我们这里还需要部署和量化，所以，这里选择 <code>[all]</code>。然后可以再检查一下 lmdeploy 包，如下图所示。</p>
<p><img src="add3.png" alt></p>
<p>基础环境到这里就配置好了。</p>
<ul>
<li>如果遇到<code>lmdeploy: command not found</code> 或其他问题，移步 <a target="_blank" rel="noopener" href="https://cguue83gpz.feishu.cn/docx/Noi7d5lllo6DMGxkuXwclxXMn5f#H2w9drpHiogeOHxhK7PcdJCmn8c">QA 文档</a></li>
</ul>
<h2 id="2-服务部署"><a href="#2-服务部署" class="headerlink" title="2 服务部署"></a>2 服务部署</h2><p>这一部分主要涉及本地推理和部署。我们先看一张图。</p>
<p><img src="lmdeploy.drawio.png" alt></p>
<p>我们把从架构上把整个服务流程分成下面几个模块。</p>
<ul>
<li>模型推理/服务。主要提供模型本身的推理，一般来说可以和具体业务解耦，专注模型推理本身性能的优化。可以以模块、API等多种方式提供。</li>
<li>Client。可以理解为前端，与用户交互的地方。</li>
<li>API Server。一般作为前端的后端，提供与产品和服务相关的数据和功能支持。</li>
</ul>
<p>值得说明的是，以上的划分是一个相对完整的模型，但在实际中这并不是绝对的。比如可以把“模型推理”和“API Server”合并，有的甚至是三个流程打包在一起提供服务。</p>
<p>接下来，我们看一下lmdeploy提供的部署功能。</p>
<h3 id="2-1-模型转换"><a href="#2-1-模型转换" class="headerlink" title="2.1 模型转换"></a>2.1 模型转换</h3><p>使用 TurboMind 推理模型需要先将模型转化为 TurboMind 的格式，目前支持在线转换和离线转换两种形式。在线转换可以直接加载 Huggingface 模型，离线转换需需要先保存模型再加载。</p>
<p>TurboMind 是一款关于 LLM 推理的高效推理引擎，基于英伟达的 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a> 研发而成。它的主要功能包括：LLaMa 结构模型的支持，persistent batch 推理模式和可扩展的 KV 缓存管理器。</p>
<h4 id="2-1-1-在线转换"><a href="#2-1-1-在线转换" class="headerlink" title="2.1.1 在线转换"></a>2.1.1 在线转换</h4><p>lmdeploy 支持直接读取 Huggingface 模型权重，目前共支持三种类型：</p>
<ul>
<li>在 huggingface.co 上面通过 lmdeploy 量化的模型，如 <a target="_blank" rel="noopener" href="https://huggingface.co/lmdeploy/llama2-chat-70b-4bit">llama2-70b-4bit</a>, <a target="_blank" rel="noopener" href="https://huggingface.co/internlm/internlm-chat-20b-4bit">internlm-chat-20b-4bit</a></li>
<li>huggingface.co 上面其他 LM 模型，如 Qwen/Qwen-7B-Chat</li>
</ul>
<p>示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要能访问 Huggingface 的网络环境</span></span><br><span class="line">lmdeploy chat turbomind internlm/internlm-chat-20b-4bit --model-name internlm-chat-20b</span><br><span class="line">lmdeploy chat turbomind Qwen/Qwen-7B-Chat --model-name qwen-7b</span><br></pre></td></tr></table></figure>
<p>上面两行命令分别展示了如何直接加载 Huggingface 的模型，第一条命令是加载使用 lmdeploy 量化的版本，第二条命令是加载其他 LLM 模型。</p>
<p>我们也可以直接启动本地的 Huggingface 模型，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b</span><br></pre></td></tr></table></figure>
<p>以上命令都会启动一个本地对话界面，通过 Bash 可以与 LLM 进行对话。</p>
<h4 id="2-1-2-离线转换"><a href="#2-1-2-离线转换" class="headerlink" title="2.1.2 离线转换"></a>2.1.2 离线转换</h4><p>离线转换需要在启动服务之前，将模型转为 lmdeploy TurboMind  的格式，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转换模型（FastTransformer格式） TurboMind</span></span><br><span class="line">lmdeploy convert internlm-chat-7b /path/to/internlm-chat-7b</span><br></pre></td></tr></table></figure>
<p>这里我们使用官方提供的模型文件，就在用户根目录执行，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/</span><br></pre></td></tr></table></figure>
<p>执行完成后将会在当前目录生成一个 <code>workspace</code> 的文件夹。这里面包含的就是 TurboMind 和 Triton “模型推理”需要到的文件。</p>
<p>目录如下图所示。</p>
<p><img src="4.png" alt></p>
<p><code>weights</code> 和 <code>tokenizer</code> 目录分别放的是拆分后的参数和 Tokenizer。如果我们进一步查看 <code>weights</code> 的目录，就会发现参数是按层和模块拆开的，如下图所示。</p>
<p><img src="5.png" alt></p>
<p>每一份参数第一个 0 表示“层”的索引，后面的那个0表示 Tensor 并行的索引，因为我们只有一张卡，所以被拆分成 1 份。如果有两张卡可以用来推理，则会生成0和1两份，也就是说，会把同一个参数拆成两份。比如 <code>layers.0.attention.w_qkv.0.weight</code> 会变成 <code>layers.0.attention.w_qkv.0.weight</code> 和 <code>layers.0.attention.w_qkv.1.weight</code>。执行 <code>lmdeploy convert</code> 命令时，可以通过 <code>--tp</code> 指定（tp 表示 tensor parallel），该参数默认值为1（也就是一张卡）。</p>
<p><strong>关于Tensor并行</strong></p>
<p>Tensor并行一般分为行并行或列并行，原理如下图所示。</p>
<p><img src="6.png" alt></p>
<p></p><p align="center">列并行</p><p></p>
<p><img src="7.png" alt></p>
<p></p><p align="center">行并行</p><p></p>
<p>简单来说，就是把一个大的张量（参数）分到多张卡上，分别计算各部分的结果，然后再同步汇总。</p>
<h3 id="2-2-TurboMind-推理-命令行本地对话"><a href="#2-2-TurboMind-推理-命令行本地对话" class="headerlink" title="2.2  TurboMind 推理+命令行本地对话"></a>2.2  TurboMind 推理+命令行本地对话</h3><p>模型转换完成后，我们就具备了使用模型推理的条件，接下来就可以进行真正的模型推理环节。</p>
<p>我们先尝试本地对话（<code>Bash Local Chat</code>），下面用（Local Chat 表示）在这里其实是跳过 API Server 直接调用 TurboMind。简单来说，就是命令行代码直接执行 TurboMind。所以说，实际和前面的架构图是有区别的。</p>
<p>这里支持多种方式运行，比如Turbomind、PyTorch、DeepSpeed。但 PyTorch 和 DeepSpeed 调用的其实都是 Huggingface 的 Transformers 包，PyTorch表示原生的 Transformer 包，DeepSpeed 表示使用了 DeepSpeed 作为推理框架。Pytorch/DeepSpeed 目前功能都比较弱，不具备生产能力，不推荐使用。</p>
<p>执行命令如下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turbomind + Bash Local Chat</span></span><br><span class="line">lmdeploy chat turbomind ./workspace</span><br></pre></td></tr></table></figure>
<p>启动后就可以和它进行对话了，如下图所示。</p>
<p><img src="8.png" alt></p>
<p>输入后两次回车，退出时输入<code>exit</code> 回车两次即可。此时，Server 就是本地跑起来的模型（TurboMind），命令行可以看作是前端。</p>
<h3 id="2-3-TurboMind推理-API服务"><a href="#2-3-TurboMind推理-API服务" class="headerlink" title="2.3 TurboMind推理+API服务"></a>2.3 TurboMind推理+API服务</h3><p>在上面的部分我们尝试了直接用命令行启动 Client，接下来我们尝试如何运用 lmdepoy 进行服务化。</p>
<p>”模型推理/服务“目前提供了 Turbomind 和 TritonServer 两种服务化方式。此时，Server 是 TurboMind 或 TritonServer，API Server 可以提供对外的 API 服务。我们推荐使用 TurboMind，TritonServer 使用方式详见《附录1》。</p>
<p>首先，通过下面命令启动服务。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ApiServer+Turbomind   api_server =&gt; AsyncEngine =&gt; TurboMind</span></span><br><span class="line">lmdeploy serve api_server ./workspace \</span><br><span class="line">	--server_name 0.0.0.0 \</span><br><span class="line">	--server_port 23333 \</span><br><span class="line">	--instance_num 64 \</span><br><span class="line">	--tp 1</span><br></pre></td></tr></table></figure>
<p>上面的参数中 <code>server_name</code> 和 <code>server_port</code> 分别表示服务地址和端口，<code>tp</code> 参数我们之前已经提到过了，表示 Tensor 并行。还剩下一个 <code>instance_num</code> 参数，表示实例数，可以理解成 Batch 的大小。执行后如下图所示。</p>
<p><img src="11.png" alt></p>
<p>然后，我们可以新开一个窗口，执行下面的 Client 命令。如果使用官方机器，可以打开 vscode 的 Terminal，执行下面的命令。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ChatApiClient+ApiServer（注意是http协议，需要加http）</span></span><br><span class="line">lmdeploy serve api_client http://localhost:23333</span><br></pre></td></tr></table></figure>
<p>如下图所示。</p>
<p><img src="12.png" alt></p>
<p>当然，刚刚我们启动的是 API Server，自然也有相应的接口。可以直接打开 <code>http://&#123;host&#125;:23333</code> 查看，如下图所示。</p>
<p><img src="13.png" alt></p>
<blockquote>
<p>注意，这一步由于 Server 在远程服务器上，所以本地需要做一下 ssh 转发才能直接访问（与第一部分操作一样），命令如下：</p>
<p>ssh -CNg -L 23333:127.0.0.1:23333 root@ssh.intern-ai.org.cn -p &lt;你的ssh端口号&gt;</p>
<p>而执行本命令需要添加本机公钥，公钥添加后等待几分钟即可生效。ssh 端口号就是下面图片里的 33087。</p>
<p><img src="20.png" alt></p>
</blockquote>
<p>这里一共提供了 4 个 HTTP 的接口，任何语言都可以对其进行调用，我们以 <code>v1/chat/completions</code> 接口为例，简单试一下。</p>
<p>接口请求参数如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;internlm-chat-7b&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;messages&quot;</span><span class="punctuation">:</span> <span class="string">&quot;写一首春天的诗&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;temperature&quot;</span><span class="punctuation">:</span> <span class="number">0.7</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;top_p&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;n&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;max_tokens&quot;</span><span class="punctuation">:</span> <span class="number">512</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;stop&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;presence_penalty&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;frequency_penalty&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;user&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;repetition_penalty&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;renew_session&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ignore_eos&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>请求结果如下。</p>
<p><img src="16.png" alt></p>
<h3 id="2-4-网页-Demo-演示"><a href="#2-4-网页-Demo-演示" class="headerlink" title="2.4 网页 Demo 演示"></a>2.4 网页 Demo 演示</h3><p>这一部分主要是将 Gradio 作为前端 Demo 演示。在上一节的基础上，我们不执行后面的 <code>api_client</code> 或 <code>triton_client</code>，而是执行 <code>gradio</code>。</p>
<blockquote>
<p>由于 Gradio 需要本地访问展示界面，因此也需要通过 ssh 将数据转发到本地。命令如下：</p>
<p>ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p &lt;你的 ssh 端口号&gt;</p>
</blockquote>
<h4 id="2-4-1-TurboMind-服务作为后端"><a href="#2-4-1-TurboMind-服务作为后端" class="headerlink" title="2.4.1 TurboMind 服务作为后端"></a>2.4.1 TurboMind 服务作为后端</h4><p>API Server 的启动和上一节一样，这里直接启动作为前端的 Gradio。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gradio+ApiServer。必须先开启 Server，此时 Gradio 为 Client</span></span><br><span class="line">lmdeploy serve gradio http://0.0.0.0:23333 \</span><br><span class="line">	--server_name 0.0.0.0 \</span><br><span class="line">	--server_port 6006 \</span><br><span class="line">	--restful_api True</span><br></pre></td></tr></table></figure>
<p>结果如下图所示。</p>
<p><img src="17.png" alt></p>
<h4 id="2-4-2-TurboMind-推理作为后端"><a href="#2-4-2-TurboMind-推理作为后端" class="headerlink" title="2.4.2 TurboMind 推理作为后端"></a>2.4.2 TurboMind 推理作为后端</h4><p>当然，Gradio 也可以直接和 TurboMind 连接，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gradio+Turbomind(local)</span></span><br><span class="line">lmdeploy serve gradio ./workspace</span><br></pre></td></tr></table></figure>
<p>可以直接启动 Gradio，此时没有 API Server，TurboMind 直接与 Gradio 通信。如下图所示。</p>
<p><img src="19.png" alt></p>
<h3 id="2-5-TurboMind-推理-Python-代码集成"><a href="#2-5-TurboMind-推理-Python-代码集成" class="headerlink" title="2.5 TurboMind 推理 + Python 代码集成"></a>2.5 TurboMind 推理 + Python 代码集成</h3><p>前面介绍的都是通过 API 或某种前端与”模型推理/服务“进行交互，lmdeploy 还支持 Python 直接与 TurboMind 进行交互，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lmdeploy <span class="keyword">import</span> turbomind <span class="keyword">as</span> tm</span><br><span class="line"></span><br><span class="line"><span class="comment"># load model</span></span><br><span class="line">model_path = <span class="string">&quot;/root/share/temp/model_repos/internlm-chat-7b/&quot;</span></span><br><span class="line">tm_model = tm.TurboMind.from_pretrained(model_path, model_name=<span class="string">&#x27;internlm-chat-20b&#x27;</span>)</span><br><span class="line">generator = tm_model.create_instance()</span><br><span class="line"></span><br><span class="line"><span class="comment"># process query</span></span><br><span class="line">query = <span class="string">&quot;你好啊兄嘚&quot;</span></span><br><span class="line">prompt = tm_model.model.get_prompt(query)</span><br><span class="line">input_ids = tm_model.tokenizer.encode(prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inference</span></span><br><span class="line"><span class="keyword">for</span> outputs <span class="keyword">in</span> generator.stream_infer(</span><br><span class="line">        session_id=<span class="number">0</span>,</span><br><span class="line">        input_ids=[input_ids]):</span><br><span class="line">    res, tokens = outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">response = tm_model.tokenizer.decode(res.tolist())</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>在上面的代码中，我们首先加载模型，然后构造输入，最后执行推理。</p>
<p>加载模型可以显式指定模型路径，也可以直接指定 Huggingface 的 repo_id，还可以使用上面生成过的 <code>workspace</code>。这里的 <code>tm.TurboMind</code> 其实是对 C++ TurboMind 的封装。</p>
<p>构造输入这里主要是把用户的 query 构造成 InternLLM 支持的输入格式，比如上面的例子中， <code>query</code> 是“你好啊兄嘚”，构造好的 Prompt 如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;|System|&gt;:You are an AI assistant whose name is InternLM (书生·浦语).</span></span><br><span class="line"><span class="string">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span></span><br><span class="line"><span class="string">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;|User|&gt;:你好啊兄嘚</span></span><br><span class="line"><span class="string">&lt;|Bot|&gt;:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Prompt 其实就是增加了 <code>&lt;|System|&gt;</code> 消息和 <code>&lt;|User|&gt;</code> 消息（即用户的 <code>query</code>），以及一个 <code>&lt;|Bot|&gt;</code> 的标记，表示接下来该模型输出响应了。最终输出的响应内容如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;你好啊，有什么我可以帮助你的吗？&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-6-这么多，头秃，有没有最佳实践"><a href="#2-6-这么多，头秃，有没有最佳实践" class="headerlink" title="2.6 这么多，头秃，有没有最佳实践"></a>2.6 这么多，头秃，有没有最佳实践</h3><h4 id="2-6-1-方案实践"><a href="#2-6-1-方案实践" class="headerlink" title="2.6.1 方案实践"></a>2.6.1 方案实践</h4><p>必——须——有！</p>
<p>首先说 “模型推理/服务”，推荐使用 TurboMind，使用简单，性能良好，相关的 Benchmark 对比如下。</p>
<p><img src="add4.png" alt></p>
<p>上面的性能对比包括两个场景：</p>
<ul>
<li>场景一（前4张图）：固定的输入、输出 token 数（分别1和2048），测试Token输出吞吐量（output token throughput）。</li>
<li>场景二（第5张图）：使用真实数据，测试吞吐量（request throughput）。</li>
</ul>
<p>场景一中，BatchSize=64时，TurboMind 的吞吐量超过 2000 token/s，整体比 DeepSpeed 提升约 5% - 15%；BatchSize=32时，比 Huggingface 的Transformers 提升约 3 倍；其他BatchSize时 TurboMind 也表现出优异的性能。</p>
<p>场景二中，对比了 TurboMind 和 vLLM 在真实数据上的吞吐量（request throughput）指标，TurboMind 的效率比 vLLM 高 30%。</p>
<p>大家不妨亲自使用本地对话（Local Chat）感受一下性能差别（2.2 节），也可以执行我们提供的 <code>infer_compare.py</code> 脚本，示例如下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行 Huggingface 的 Transformer</span></span><br><span class="line">python infer_compare.py hf</span><br><span class="line"><span class="comment"># 执行LMDeploy</span></span><br><span class="line">python infer_compare.py lmdeploy</span><br></pre></td></tr></table></figure>
<p>LMDeploy应该是Transformers的3-5倍左右。</p>
<p>后面的 API 服务和 Client 就得分场景了。</p>
<ul>
<li>我想对外提供类似 OpenAI 那样的 HTTP 接口服务。推荐使用 TurboMind推理 + API 服务（2.3）。</li>
<li>我想做一个演示 Demo，Gradio 无疑是比 Local Chat 更友好的。推荐使用 TurboMind 推理作为后端的Gradio进行演示（2.4.2）。</li>
<li>我想直接在自己的 Python 项目中使用大模型功能。推荐使用 TurboMind推理 + Python（2.5）。</li>
<li>我想在自己的其他非 Python 项目中使用大模型功能。推荐直接通过 HTTP 接口调用服务。也就是用本列表第一条先启动一个 HTTP API 服务，然后在项目中直接调用接口。</li>
<li>我的项目是 C++ 写的，为什么不能直接用 TurboMind 的 C++ 接口？！必须可以！大佬可以右上角叉掉这个窗口啦。</li>
</ul>
<h4 id="2-6-2-模型配置实践"><a href="#2-6-2-模型配置实践" class="headerlink" title="2.6.2 模型配置实践"></a>2.6.2 模型配置实践</h4><p>不知道大家还有没有印象，在离线转换（2.1.2）一节，我们查看了 <code>weights</code> 的目录，里面存放的是模型按层、按并行卡拆分的参数，不过还有一个文件 <code>config.ini</code> 并不是模型参数，它里面存的主要是模型相关的配置信息。下面是一个示例。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[llama]</span></span><br><span class="line"><span class="attr">model_name</span> = internlm-chat-<span class="number">7</span>b</span><br><span class="line"><span class="attr">tensor_para_size</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">head_num</span> = <span class="number">32</span></span><br><span class="line"><span class="attr">kv_head_num</span> = <span class="number">32</span></span><br><span class="line"><span class="attr">vocab_size</span> = <span class="number">103168</span></span><br><span class="line"><span class="attr">num_layer</span> = <span class="number">32</span></span><br><span class="line"><span class="attr">inter_size</span> = <span class="number">11008</span></span><br><span class="line"><span class="attr">norm_eps</span> = <span class="number">1</span>e-<span class="number">06</span></span><br><span class="line"><span class="attr">attn_bias</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">start_id</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">end_id</span> = <span class="number">2</span></span><br><span class="line"><span class="attr">session_len</span> = <span class="number">2056</span></span><br><span class="line"><span class="attr">weight_type</span> = fp16</span><br><span class="line"><span class="attr">rotary_embedding</span> = <span class="number">128</span></span><br><span class="line"><span class="attr">rope_theta</span> = <span class="number">10000.0</span></span><br><span class="line"><span class="attr">size_per_head</span> = <span class="number">128</span></span><br><span class="line"><span class="attr">group_size</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">max_batch_size</span> = <span class="number">64</span></span><br><span class="line"><span class="attr">max_context_token_num</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">step_length</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">cache_max_entry_count</span> = <span class="number">0.5</span></span><br><span class="line"><span class="attr">cache_block_seq_len</span> = <span class="number">128</span></span><br><span class="line"><span class="attr">cache_chunk_size</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">use_context_fmha</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">quant_policy</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">max_position_embeddings</span> = <span class="number">2048</span></span><br><span class="line"><span class="attr">rope_scaling_factor</span> = <span class="number">0.0</span></span><br><span class="line"><span class="attr">use_logn_attn</span> = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>其中，模型属性相关的参数不可更改，主要包括下面这些。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model_name</span> = llama2</span><br><span class="line"><span class="attr">head_num</span> = <span class="number">32</span></span><br><span class="line"><span class="attr">kv_head_num</span> = <span class="number">32</span></span><br><span class="line"><span class="attr">vocab_size</span> = <span class="number">103168</span></span><br><span class="line"><span class="attr">num_layer</span> = <span class="number">32</span></span><br><span class="line"><span class="attr">inter_size</span> = <span class="number">11008</span></span><br><span class="line"><span class="attr">norm_eps</span> = <span class="number">1</span>e-<span class="number">06</span></span><br><span class="line"><span class="attr">attn_bias</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">start_id</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">end_id</span> = <span class="number">2</span></span><br><span class="line"><span class="attr">rotary_embedding</span> = <span class="number">128</span></span><br><span class="line"><span class="attr">rope_theta</span> = <span class="number">10000.0</span></span><br><span class="line"><span class="attr">size_per_head</span> = <span class="number">128</span></span><br></pre></td></tr></table></figure>
<p>和数据类型相关的参数也不可更改，主要包括两个。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">weight_type</span> = fp16</span><br><span class="line"><span class="attr">group_size</span> = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p><code>weight_type</code> 表示权重的数据类型。目前支持 fp16 和 int4。int4 表示 4bit 权重。当 <code>weight_type</code> 为 4bit 权重时，<code>group_size</code> 表示 <code>awq</code> 量化权重时使用的 group 大小。</p>
<p>剩余参数包括下面几个。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tensor_para_size</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">session_len</span> = <span class="number">2056</span></span><br><span class="line"><span class="attr">max_batch_size</span> = <span class="number">64</span></span><br><span class="line"><span class="attr">max_context_token_num</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">step_length</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">cache_max_entry_count</span> = <span class="number">0.5</span></span><br><span class="line"><span class="attr">cache_block_seq_len</span> = <span class="number">128</span></span><br><span class="line"><span class="attr">cache_chunk_size</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">use_context_fmha</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">quant_policy</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">max_position_embeddings</span> = <span class="number">2048</span></span><br><span class="line"><span class="attr">rope_scaling_factor</span> = <span class="number">0.0</span></span><br><span class="line"><span class="attr">use_logn_attn</span> = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>一般情况下，我们并不需要对这些参数进行修改，但有时候为了满足特定需要，可能需要调整其中一部分配置值。这里主要介绍三个可能需要调整的参数。</p>
<ul>
<li>KV int8 开关：<ul>
<li>对应参数为 <code>quant_policy</code>，默认值为 0，表示不使用 KV Cache，如果需要开启，则将该参数设置为 4。</li>
<li>KV Cache 是对序列生成过程中的 K 和 V 进行量化，用以节省显存。我们下一部分会介绍具体的量化过程。</li>
<li>当显存不足，或序列比较长时，建议打开此开关。</li>
</ul>
</li>
<li>外推能力开关：<ul>
<li>对应参数为 <code>rope_scaling_factor</code>，默认值为 0.0，表示不具备外推能力，设置为 1.0，可以开启 RoPE 的 Dynamic NTK 功能，支持长文本推理。另外，<code>use_logn_attn</code> 参数表示 Attention 缩放，默认值为 0，如果要开启，可以将其改为 1。</li>
<li>外推能力是指推理时上下文的长度超过训练时的最大长度时模型生成的能力。如果没有外推能力，当推理时上下文长度超过训练时的最大长度，效果会急剧下降。相反，则下降不那么明显，当然如果超出太多，效果也会下降的厉害。</li>
<li>当推理文本非常长（明显超过了训练时的最大长度）时，建议开启外推能力。</li>
</ul>
</li>
<li>批处理大小：<ul>
<li>对应参数为 <code>max_batch_size</code>，默认为 64，也就是我们在 API Server 启动时的 <code>instance_num</code> 参数。</li>
<li>该参数值越大，吞度量越大（同时接受的请求数），但也会占用更多显存。</li>
<li>建议根据请求量和最大的上下文长度，按实际情况调整。</li>
</ul>
</li>
</ul>
<h2 id="3-模型量化"><a href="#3-模型量化" class="headerlink" title="3 模型量化"></a>3 模型量化</h2><p>本部分内容主要介绍如何对模型进行量化。主要包括 KV Cache 量化和模型参数量化。总的来说，量化是一种以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）的策略。</p>
<p>正式介绍 LMDeploy 量化方案前，需要先介绍两个概念：</p>
<ul>
<li>计算密集（compute-bound）: 指推理过程中，绝大部分时间消耗在数值计算上；针对计算密集型场景，可以通过使用更快的硬件计算单元来提升计算速。</li>
<li>访存密集（memory-bound）: 指推理过程中，绝大部分时间消耗在数据读取上；针对访存密集型场景，一般通过减少访存次数、提高计算访存比或降低访存量来优化。</li>
</ul>
<p>常见的 LLM 模型由于 Decoder Only 架构的特性，实际推理时大多数的时间都消耗在了逐 Token 生成阶段（Decoding 阶段），是典型的访存密集型场景。</p>
<p>那么，如何优化 LLM 模型推理中的访存密集问题呢？ 我们可以使用 <strong>KV Cache 量化</strong>和 <strong>4bit Weight Only 量化（W4A16）</strong>。KV Cache 量化是指将逐 Token（Decoding）生成过程中的上下文 K 和 V 中间结果进行 INT8 量化（计算时再反量化），以降低生成过程中的显存占用。4bit Weight 量化，将 FP16 的模型权重量化为 INT4，Kernel 计算时，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本。Weight Only 是指仅量化权重，数值计算依然采用 FP16（需要将 INT4 权重反量化）。</p>
<h3 id="3-1-KV-Cache-量化"><a href="#3-1-KV-Cache-量化" class="headerlink" title="3.1 KV Cache 量化"></a>3.1 KV Cache 量化</h3><h4 id="3-1-1-量化步骤"><a href="#3-1-1-量化步骤" class="headerlink" title="3.1.1 量化步骤"></a>3.1.1 量化步骤</h4><p>KV Cache 量化是将已经生成序列的 KV 变成 Int8，使用过程一共包括三步：</p>
<p>第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。</p>
<ul>
<li>对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 <code>(num_heads, head_dim)</code> 的矩阵。这里的统计结果将用于本小节的 KV Cache。</li>
<li>对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 <code>(hidden_dim, )</code> 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同。这里的统计结果用于下个小节的模型参数量化，主要用在缩放环节（回顾PPT内容）。</li>
</ul>
<p>第一步执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算 minmax</span></span><br><span class="line">lmdeploy lite calibrate \</span><br><span class="line">  --model  /root/share/temp/model_repos/internlm-chat-7b/ \</span><br><span class="line">  --calib_dataset <span class="string">&quot;c4&quot;</span> \</span><br><span class="line">  --calib_samples 128 \</span><br><span class="line">  --calib_seqlen 2048 \</span><br><span class="line">  --work_dir ./quant_output</span><br></pre></td></tr></table></figure>
<p>在这个命令行中，会选择 128 条输入样本，每条样本长度为 2048，数据集选择 C4，输入模型后就会得到上面的各种统计值。值得说明的是，如果显存不足，可以适当调小 samples 的数量或 sample 的长度。</p>
<blockquote>
<p>这一步由于默认需要从 Huggingface 下载数据集，国内经常不成功。所以我们导出了需要的数据，大家需要对读取数据集的代码文件做一下替换。共包括两步：</p>
<ul>
<li>第一步：复制 <code>calib_dataloader.py</code> 到安装目录替换该文件：<code>cp /root/share/temp/datasets/c4/calib_dataloader.py  /root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/utils/</code></li>
<li>第二步：将用到的数据集（c4）复制到下面的目录：<code>cp -r /root/share/temp/datasets/c4/ /root/.cache/huggingface/datasets/</code> </li>
</ul>
</blockquote>
<p>第二步：通过 minmax 获取量化参数。主要就是利用下面这个公式，获取每一层的 K V 中心值（zp）和缩放值（scale）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zp = (min+max) / 2</span><br><span class="line">scale = (max-min) / 255</span><br><span class="line">quant: q = round( (f-zp) / scale)</span><br><span class="line">dequant: f = q * scale + zp</span><br></pre></td></tr></table></figure>
<p>有这两个值就可以进行量化和解量化操作了。具体来说，就是对历史的 K 和 V 存储 quant 后的值，使用时在 dequant。</p>
<p>第二步的执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过 minmax 获取量化参数</span></span><br><span class="line">lmdeploy lite kv_qparams \</span><br><span class="line">  --work_dir ./quant_output  \</span><br><span class="line">  --turbomind_dir workspace/triton_models/weights/ \</span><br><span class="line">  --kv_sym False \</span><br><span class="line">  --num_tp 1</span><br></pre></td></tr></table></figure>
<p>在这个命令中，<code>num_tp</code> 的含义前面介绍过，表示 Tensor 的并行数。每一层的中心值和缩放值会存储到 <code>workspace</code> 的参数目录中以便后续使用。<code>kv_sym</code> 为 <code>True</code> 时会使用另一种（对称）量化方法，它用到了第一步存储的绝对值最大值，而不是最大值和最小值。</p>
<p>第三步：修改配置。也就是修改 <code>weights/config.ini</code> 文件，这个我们在《2.6.2 模型配置实践》中已经提到过了（KV int8 开关），只需要把 <code>quant_policy</code> 改为 4 即可。</p>
<p>这一步需要额外说明的是，如果用的是 TurboMind1.0，还需要修改参数 <code>use_context_fmha</code>，将其改为 0。</p>
<p>接下来就可以正常运行前面的各种服务了，只不过咱们现在可是用上了 KV Cache 量化，能更省（运行时）显存了。</p>
<h4 id="3-1-2-量化效果"><a href="#3-1-2-量化效果" class="headerlink" title="3.1.2 量化效果"></a>3.1.2 量化效果</h4><p>官方给出了 <a target="_blank" rel="noopener" href="https://huggingface.co/internlm/internlm-chat-7b">internlm-chat-7b</a> 模型在 KV Cache 量化前后的显存对比情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>batch_size</th>
<th>fp16 memory(MiB)</th>
<th>int8 memory(MiB)</th>
<th>diff(MiB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>22337</td>
<td>18241</td>
<td>-4096</td>
</tr>
<tr>
<td>16</td>
<td>30593</td>
<td>22369</td>
<td>-8224</td>
</tr>
<tr>
<td>32</td>
<td>47073</td>
<td>30625</td>
<td>-16448</td>
</tr>
<tr>
<td>48</td>
<td>63553</td>
<td>38881</td>
<td>-24672</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，KV Cache 可以节约大约 20% 的显存。</p>
<p>同时，还在 <a target="_blank" rel="noopener" href="https://github.com/open-compass/opencompass">opencompass</a> 平台上测试了量化前后的精准度（Accuracy）对比情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>task</th>
<th>dataset</th>
<th>metric</th>
<th>int8</th>
<th>fp16</th>
<th>diff</th>
</tr>
</thead>
<tbody>
<tr>
<td>Language</td>
<td>winogrande</td>
<td>accuracy</td>
<td>60.77</td>
<td>61.48</td>
<td>-0.71</td>
</tr>
<tr>
<td>Knowledge</td>
<td>nq</td>
<td>score</td>
<td>2.69</td>
<td>2.60</td>
<td>+0.09</td>
</tr>
<tr>
<td>Reasoning</td>
<td>gsm8k</td>
<td>accuracy</td>
<td>33.28</td>
<td>34.72</td>
<td>-1.44</td>
</tr>
<tr>
<td>Reasoning</td>
<td>bbh</td>
<td>naive_average</td>
<td>20.12</td>
<td>20.51</td>
<td>-0.39</td>
</tr>
<tr>
<td>Understanding</td>
<td>openbookqa_fact</td>
<td>accuracy</td>
<td>82.40</td>
<td>82.20</td>
<td>+0.20</td>
</tr>
<tr>
<td>Understanding</td>
<td>eprstmt-dev</td>
<td>accuracy</td>
<td>90.62</td>
<td>88.75</td>
<td>+1.87</td>
</tr>
<tr>
<td>Safety</td>
<td>crows_pairs</td>
<td>accuracy</td>
<td>32.56</td>
<td>31.43</td>
<td>+1.13</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，精度不仅没有明显下降，相反在不少任务上还有一定的提升。可能得原因是，量化会导致一定的误差，有时候这种误差可能会减少模型对训练数据的拟合，从而提高泛化性能。量化可以被视为引入轻微噪声的正则化方法。或者，也有可能量化后的模型正好对某些数据集具有更好的性能。</p>
<p>总结一下，KV Cache 量化既能明显降低显存占用，还有可能同时带来精准度（Accuracy）的提升。</p>
<h3 id="3-2-W4A16-量化"><a href="#3-2-W4A16-量化" class="headerlink" title="3.2 W4A16 量化"></a>3.2 W4A16 量化</h3><h4 id="3-2-1-量化步骤"><a href="#3-2-1-量化步骤" class="headerlink" title="3.2.1 量化步骤"></a>3.2.1 量化步骤</h4><p>W4A16中的A是指Activation，保持FP16，只对参数进行 4bit 量化。使用过程也可以看作是三步。</p>
<p>第一步：同 1.3.1，不再赘述。</p>
<p>第二步：量化权重模型。利用第一步得到的统计值对参数进行量化，具体又包括两小步：</p>
<ul>
<li>缩放参数。主要是性能上的考虑（回顾 PPT）。</li>
<li>整体量化。</li>
</ul>
<p>第二步的执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 量化权重模型</span></span><br><span class="line">lmdeploy lite auto_awq \</span><br><span class="line">  --model  /root/share/temp/model_repos/internlm-chat-7b/ \</span><br><span class="line">  --w_bits 4 \</span><br><span class="line">  --w_group_size 128 \</span><br><span class="line">  --work_dir ./quant_output </span><br></pre></td></tr></table></figure>
<p>命令中 <code>w_bits</code> 表示量化的位数，<code>w_group_size</code> 表示量化分组统计的尺寸，<code>work_dir</code> 是量化后模型输出的位置。这里需要特别说明的是，因为没有 <code>torch.int4</code>，所以实际存储时，8个 4bit 权重会被打包到一个 int32 值中。所以，如果你把这部分量化后的参数加载进来就会发现它们是 int32 类型的。</p>
<p>最后一步：转换成 TurboMind 格式。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转换模型的layout，存放在默认路径 ./workspace 下</span></span><br><span class="line">lmdeploy convert  internlm-chat-7b ./quant_output \</span><br><span class="line">    --model-format awq \</span><br><span class="line">    --group-size 128</span><br></pre></td></tr></table></figure>
<p>这个 <code>group-size</code> 就是上一步的那个 <code>w_group_size</code>。如果不想和之前的 <code>workspace</code> 重复，可以指定输出目录：<code>--dst_path</code>，比如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy convert  internlm-chat-7b ./quant_output \</span><br><span class="line">    --model-format awq \</span><br><span class="line">    --group-size 128 \</span><br><span class="line">    --dst_path ./workspace_quant</span><br></pre></td></tr></table></figure>
<p>接下来和上一节一样，可以正常运行前面的各种服务了，不过咱们现在用的是量化后的模型。</p>
<p>最后再补充一点，量化模型和 KV Cache 量化也可以一起使用，以达到最大限度节省显存。</p>
<h4 id="3-2-2-量化效果"><a href="#3-2-2-量化效果" class="headerlink" title="3.2.2 量化效果"></a>3.2.2 量化效果</h4><p>官方在 NVIDIA GeForce RTX 4090 上测试了 4-bit 的 Llama-2-7B-chat 和 Llama-2-13B-chat 模型的 token 生成速度。测试配置为 BatchSize = 1，prompt_tokens=1，completion_tokens=512，结果如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>model</th>
<th>llm-awq</th>
<th>mlc-llm</th>
<th>turbomind</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-7B-chat</td>
<td>112.9</td>
<td>159.4</td>
<td>206.4</td>
</tr>
<tr>
<td>Llama-2-13B-chat</td>
<td>N/A</td>
<td>90.7</td>
<td>115.8</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，TurboMind 相比其他框架速度优势非常显著，比 mlc-llm 快了将近 30%。</p>
<p>另外，也测试了 TurboMind 在不同精度和上下文长度下的显存占用情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>model(context length)</th>
<th>16bit(2048)</th>
<th>4bit(2048)</th>
<th>16bit(4096)</th>
<th>4bit(4096)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-7B-chat</td>
<td>15.1</td>
<td>6.3</td>
<td>16.2</td>
<td>7.5</td>
</tr>
<tr>
<td>Llama-2-13B-chat</td>
<td>OOM</td>
<td>10.3</td>
<td>OOM</td>
<td>12.0</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，4bit 模型可以降低 50-60% 的显存占用，效果非常明显。</p>
<p>总而言之，W4A16 参数量化后能极大地降低显存，同时相比其他框架推理速度具有明显优势。</p>
<h3 id="3-3-最佳实践"><a href="#3-3-最佳实践" class="headerlink" title="3.3 最佳实践"></a>3.3 最佳实践</h3><p>本节是针对《模型量化》部分的最佳实践。</p>
<p>首先我们需要明白一点，服务部署和量化是没有直接关联的，量化的最主要目的是降低显存占用，主要包括两方面的显存：模型参数和中间过程计算结果。前者对应《3.2 W4A16 量化》，后者对应《3.1 KV Cache 量化》。</p>
<p>量化在降低显存的同时，一般还能带来性能的提升，因为更小精度的浮点数要比高精度的浮点数计算效率高，而整型要比浮点数高很多。</p>
<p>所以我们的建议是：在各种配置下尝试，看效果能否满足需要。这一般需要在自己的数据集上进行测试。具体步骤如下。</p>
<ul>
<li>Step1：优先尝试正常（非量化）版本，评估效果。<ul>
<li>如果效果不行，需要尝试更大参数模型或者微调。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step2：尝试正常版本+KV Cache 量化，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step3：尝试量化版本，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step4：尝试量化版本+ KV Cache 量化，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，使用方案。</li>
</ul>
</li>
</ul>
<p>简单流程如下图所示。</p>
<p><img src="quant.drawio.png" alt></p>
<p>另外需要补充说明的是，使用哪种量化版本、开启哪些功能，除了上述流程外，<strong>还需要考虑框架、显卡的支持情况</strong>，比如有些框架可能不支持 W4A16 的推理，那即便转换好了也用不了。</p>
<p>根据实践经验，一般情况下：</p>
<ul>
<li>精度越高，显存占用越多，推理效率越低，但一般效果较好。</li>
<li>Server 端推理一般用非量化版本或半精度、BF16、Int8 等精度的量化版本，比较少使用更低精度的量化版本。</li>
<li>端侧推理一般都使用量化版本，且大多是低精度的量化版本。这主要是因为计算资源所限。</li>
</ul>
<p>以上是针对项目开发情况，如果是自己尝试（玩儿）的话：</p>
<ul>
<li>如果资源足够（有GPU卡很重要），那就用非量化的正常版本。</li>
<li>如果没有 GPU 卡，只有 CPU（不管什么芯片），那还是尝试量化版本。</li>
<li>如果生成文本长度很长，显存不够，就开启 KV Cache。</li>
</ul>
<p>建议大家根据实际情况灵活选择方案。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/InternLM/lmdeploy/">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/665725861">仅需一块 3090 显卡，高效部署 InternLM-20B 模型 - 知乎</a></li>
</ul>
<h2 id="附录1：TritonServer-作为推理引擎"><a href="#附录1：TritonServer-作为推理引擎" class="headerlink" title="附录1：TritonServer 作为推理引擎"></a>附录1：TritonServer 作为推理引擎</h2><h3 id="TritonServer环境配置"><a href="#TritonServer环境配置" class="headerlink" title="TritonServer环境配置"></a>TritonServer环境配置</h3><blockquote>
<p>注意：本部分内容仅支持物理机上执行，不支持虚拟主机。</p>
</blockquote>
<p>使用 Triton Server 需要安装一下 Docker 及其他依赖。</p>
<p>先装一些基本的依赖。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install cmake sudo -y</span><br></pre></td></tr></table></figure>
<p>然后是 Docker 安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add Docker&#x27;s official GPG key:</span></span><br><span class="line">sudo apt-get install ca-certificates curl gnupg</span><br><span class="line">sudo install -m 0755 -d /etc/apt/keyrings</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg</span><br><span class="line">sudo <span class="built_in">chmod</span> a+r /etc/apt/keyrings/docker.gpg</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the repository to Apt sources:</span></span><br><span class="line"><span class="built_in">echo</span> \</span><br><span class="line">  <span class="string">&quot;deb [arch=<span class="subst">$(dpkg --print-architecture)</span> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string">  <span class="subst">$(. /etc/os-release &amp;&amp; echo <span class="string">&quot;<span class="variable">$VERSION_CODENAME</span>&quot;</span>)</span> stable&quot;</span> | \</span><br><span class="line">  sudo <span class="built_in">tee</span> /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment"># install</span></span><br><span class="line">sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</span><br></pre></td></tr></table></figure>
<p>安装后我们跑一个 HelloWorld。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># helloworld</span></span><br><span class="line">sudo docker run hello-world</span><br></pre></td></tr></table></figure>
<p>可以看到类似下面的画面，表示运行成功。</p>
<p><img src="3.png" alt></p>
<h3 id="TritonServer推理-API服务"><a href="#TritonServer推理-API服务" class="headerlink" title="TritonServer推理+API服务"></a>TritonServer推理+API服务</h3><blockquote>
<p>注意：这部分需要 Docker 服务。</p>
</blockquote>
<p>这里我们把提供模型推理服务的引擎从 TurboMind 换成了 TritonServer，启动命令就一行。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ApiServer+Triton</span></span><br><span class="line">bash workspace/service_docker_up.sh</span><br></pre></td></tr></table></figure>
<p>这里会启动一个 TritonServer 的容器，如下图所示。</p>
<p><img src="14.png" alt></p>
<p>可以再开一个窗口执行 Client 命令。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ChatTritonClient + TritonServer（注意是gRPC协议，不要用http）</span></span><br><span class="line">lmdeploy serve triton_client  localhost:33337</span><br></pre></td></tr></table></figure>
<p>结果如下图所示。</p>
<p><img src="15.png" alt></p>
<h3 id="TritonServer-服务作为后端"><a href="#TritonServer-服务作为后端" class="headerlink" title="TritonServer 服务作为后端"></a>TritonServer 服务作为后端</h3><p>使用过程同 2.4.1 小节。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gradio+TritonServer（注意是gRPC协议，不要用http）</span></span><br><span class="line">lmdeploy serve gradio localhost:33337 \</span><br><span class="line">	--server_name 0.0.0.0 \</span><br><span class="line">	--server_port 6006</span><br></pre></td></tr></table></figure>
<p>结果如下图所示。</p>
<p><img src="18.png" alt></p>
<h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><p>提交方式：在各个班级对应的 GitHub Discussion 帖子中进行提交。 </p>
<p><strong>基础作业：</strong></p>
<ul>
<li>使用 LMDeploy 以本地对话、网页Gradio、API服务中的一种方式部署 InternLM-Chat-7B 模型，生成 300 字的小故事（需截图）</li>
</ul>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/poster/fa4489c9.html"
      title="lesson6_OpenCompass大模型测评"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        lesson6_OpenCompass大模型测评
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/poster/4bdca450.html"
      title="lesson4作业"
     >

    <p class="title-text">
      
        lesson4作业
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>


  
  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <div id="comment-card" class="comment-card">
    <div class="main-title-bar">
      <div class="main-title-dot"></div>
      <div class="main-title">Comments </div>
    </div>
    <div id="vcomments"></div>
  </div>
  <script>
      new Valine({"enable":true,"appId":null,"appKey":null,"placeholder":"Just go go","pageSize":10,"highlight":true,"serverURLs":null,"el":"#vcomments"});
  </script>



    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2024 aqizhoua<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
